"""
# ----------------------------------------------------------------------------
# æ–‡æœ¬ç¿»è¯‘å™¨æ¨¡å— - åŸºäºä¸Šä¸‹æ–‡çš„é«˜è´¨é‡æ™ºèƒ½ç¿»è¯‘
# 
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. åŸºäºContentSummarizerä¸Šä¸‹æ–‡çš„æ™ºèƒ½ç¿»è¯‘
# 2. åŒé˜¶æ®µç¿»è¯‘ç­–ç•¥ï¼šå¿ å®ç¿»è¯‘ â†’ è¡¨è¾¾ä¼˜åŒ–
# 3. æœ¯è¯­ä¸€è‡´æ€§ä¿è¯å’Œæ™ºèƒ½åŒ¹é…
# 4. å¹¶è¡Œæ‰¹é‡ç¿»è¯‘å’Œæ€§èƒ½ä¼˜åŒ–
# 5. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¿»è¯‘è´¨é‡æ§åˆ¶
# 6. è‡ªé€‚åº”åˆ†å—å’Œè¾¹ç•Œå¤„ç†
# 
# è¾“å…¥ï¼šåˆ†å‰²æ–‡æœ¬æ–‡ä»¶ï¼Œæœ¯è¯­ä¿¡æ¯ï¼Œç¿»è¯‘ä¸Šä¸‹æ–‡
# è¾“å‡ºï¼šé«˜è´¨é‡åŒè¯­å¯¹ç…§ç¿»è¯‘ç»“æœExcel
# 
# è®¾è®¡åŸåˆ™ï¼š
# - å……åˆ†åˆ©ç”¨ContentSummarizeræä¾›çš„ä¸°å¯Œä¸Šä¸‹æ–‡
# - ç¡®ä¿æœ¯è¯­ç¿»è¯‘çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
# - æ”¯æŒå¤šç§ç¿»è¯‘ç­–ç•¥å’Œè´¨é‡æ§åˆ¶
# - é«˜æ•ˆçš„å¹¶è¡Œå¤„ç†å’Œé”™è¯¯æ¢å¤
# ----------------------------------------------------------------------------
"""

import os
import json
import hashlib
import math
from typing import List, Dict, Optional, Callable, Tuple, Any
from pathlib import Path
import concurrent.futures
from dataclasses import dataclass, asdict
from collections import defaultdict
from difflib import SequenceMatcher
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)


@dataclass
class TranslationChunk:
    """ç¿»è¯‘å—æ•°æ®ç±»"""
    index: int                      # å—ç´¢å¼•
    source_text: str               # æºæ–‡æœ¬
    translated_text: str = ""      # ç¿»è¯‘æ–‡æœ¬
    confidence: float = 0.0        # ç¿»è¯‘ç½®ä¿¡åº¦
    processing_time: float = 0.0   # å¤„ç†æ—¶é—´
    retry_count: int = 0           # é‡è¯•æ¬¡æ•°
    error_message: str = ""        # é”™è¯¯ä¿¡æ¯
    context_terms: List[str] = None # ä½¿ç”¨çš„æœ¯è¯­
    
    def __post_init__(self):
        if self.context_terms is None:
            self.context_terms = []


@dataclass
class TranslationResult:
    """ç¿»è¯‘ç»“æœæ•°æ®ç±»"""
    total_chunks: int              # æ€»å—æ•°
    successful_chunks: int         # æˆåŠŸå—æ•°
    failed_chunks: int             # å¤±è´¥å—æ•°
    total_time: float              # æ€»å¤„ç†æ—¶é—´
    average_confidence: float      # å¹³å‡ç½®ä¿¡åº¦
    output_file: str               # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    chunks: List[TranslationChunk] = None # è¯¦ç»†ç»“æœ
    
    def __post_init__(self):
        if self.chunks is None:
            self.chunks = []


class TextTranslator:
    """æ–‡æœ¬ç¿»è¯‘å™¨ç±» - åŸºäºä¸Šä¸‹æ–‡çš„é«˜è´¨é‡ç¿»è¯‘"""
    
    def __init__(self,
                 input_file: str = 'output/log/3_2_split_by_meaning.txt',
                 terminology_file: str = 'output/log/4_1_terminology.json',
                 context_file: str = 'output/log/4_1_translation_context.json',
                 output_dir: str = 'output/log',
                 src_language: str = 'en',
                 tgt_language: str = 'zh',
                 chunk_size: int = 600,
                 max_chunk_lines: int = 10,
                 max_workers: int = 4,
                 enable_reflect_translate: bool = True,
                 similarity_threshold: float = 0.9):
        """
        åˆå§‹åŒ–æ–‡æœ¬ç¿»è¯‘å™¨
        
        Args:
            input_file: è¾“å…¥çš„åˆ†å‰²æ–‡æœ¬æ–‡ä»¶
            terminology_file: æœ¯è¯­ä¿¡æ¯æ–‡ä»¶
            context_file: ç¿»è¯‘ä¸Šä¸‹æ–‡æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•
            src_language: æºè¯­è¨€ä»£ç 
            tgt_language: ç›®æ ‡è¯­è¨€ä»£ç 
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            max_chunk_lines: åˆ†å—æœ€å¤§è¡Œæ•°
            max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°
            enable_reflect_translate: æ˜¯å¦å¯ç”¨åæ€ç¿»è¯‘
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.input_file = Path(input_file)
        self.terminology_file = Path(terminology_file)
        self.context_file = Path(context_file)
        self.output_dir = Path(output_dir)
        self.src_language = src_language
        self.tgt_language = tgt_language
        self.chunk_size = chunk_size
        self.max_chunk_lines = max_chunk_lines
        self.max_workers = max_workers
        self.enable_reflect_translate = enable_reflect_translate
        self.similarity_threshold = similarity_threshold
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.translation_file = self.output_dir / '4_2_translation.xlsx'
        self.faithful_file = self.output_dir / '4_2_faithful_translation.json'
        self.expressive_file = self.output_dir / '4_2_expressive_translation.json'
        
        # æ‡’åŠ è½½ä¾èµ–
        self._pd = None
        self._ask_gpt_func = None
        
        # å†…éƒ¨çŠ¶æ€
        self._terminology_data = {}
        self._translation_context = {}
        self._content_theme = ""
        self._chunks = []
        self._term_mapping = {}
        
    def _get_pandas(self):
        """æ‡’åŠ è½½pandas"""
        if self._pd is None:
            try:
                import pandas as pd
                self._pd = pd
            except ImportError:
                raise ImportError("âŒ æœªæ‰¾åˆ°pandasåº“, è¯·å®‰è£…: pip install pandas")
        return self._pd
    
    def set_gpt_function(self, ask_gpt_func: Callable):
        """
        è®¾ç½®GPTè°ƒç”¨å‡½æ•°
        
        Args:
            ask_gpt_func: GPT APIè°ƒç”¨å‡½æ•°ï¼Œç­¾åä¸º(prompt, resp_type='json', log_title='', valid_def=None)
        """
        self._ask_gpt_func = ask_gpt_func
        print("âœ… GPTå‡½æ•°å·²è®¾ç½®")
    
    def load_translation_context(self) -> Tuple[Dict, Dict, str]:
        """
        åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            (æœ¯è¯­ä¿¡æ¯, ç¿»è¯‘ä¸Šä¸‹æ–‡, å†…å®¹ä¸»é¢˜)
        """
        print("ğŸ“š æ­£åœ¨åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡...")
        
        # åŠ è½½æœ¯è¯­ä¿¡æ¯
        terminology_data = {}
        if self.terminology_file.exists():
            try:
                with open(self.terminology_file, 'r', encoding='utf-8') as f:
                    terminology_data = json.load(f)
                print(f"âœ… åŠ è½½äº†{len(terminology_data.get('terms', []))}ä¸ªæœ¯è¯­")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æœ¯è¯­æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡
        translation_context = {}
        if self.context_file.exists():
            try:
                with open(self.context_file, 'r', encoding='utf-8') as f:
                    translation_context = json.load(f)
                print(f"âœ… åŠ è½½äº†ç¿»è¯‘ä¸Šä¸‹æ–‡ä¿¡æ¯")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ä¸Šä¸‹æ–‡æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # æå–å†…å®¹ä¸»é¢˜
        content_theme = terminology_data.get('theme', 'è§†é¢‘å†…å®¹ç¿»è¯‘')
        
        # æ„å»ºæœ¯è¯­æ˜ å°„è¡¨
        self._build_term_mapping(terminology_data.get('terms', []))
        
        return terminology_data, translation_context, content_theme
    
    def _build_term_mapping(self, terms: List[Dict]):
        """æ„å»ºæœ¯è¯­æ˜ å°„è¡¨ä»¥ä¾¿å¿«é€ŸæŸ¥è¯¢"""
        self._term_mapping = {}
        for term in terms:
            src = term.get('src', '').lower()
            if src:
                self._term_mapping[src] = {
                    'tgt': term.get('tgt', ''),
                    'note': term.get('note', ''),
                    'category': term.get('category', ''),
                    'confidence': term.get('confidence', 1.0)
                }
        print(f"ğŸ“– æ„å»ºäº†{len(self._term_mapping)}ä¸ªæœ¯è¯­çš„å¿«é€Ÿæ˜ å°„è¡¨")
    
    def load_input_text(self) -> List[str]:
        """
        åŠ è½½è¾“å…¥æ–‡æœ¬
        
        Returns:
            æ–‡æœ¬è¡Œåˆ—è¡¨
        """
        print(f"ğŸ“– æ­£åœ¨åŠ è½½åˆ†å‰²æ–‡æœ¬: {self.input_file}")
        
        if not self.input_file.exists():
            raise FileNotFoundError(f"âŒ åˆ†å‰²æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {self.input_file}")
        
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ¸…ç†æ–‡æœ¬è¡Œ
            cleaned_lines = [line.strip() for line in lines if line.strip()]
            
            print(f"âœ… åŠ è½½äº†{len(cleaned_lines)}è¡Œæ–‡æœ¬")
            return cleaned_lines
            
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†å‰²æ–‡æœ¬å¤±è´¥: {str(e)}")
            raise
    
    def split_text_into_chunks(self, lines: List[str]) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºç¿»è¯‘å—
        
        Args:
            lines: æ–‡æœ¬è¡Œåˆ—è¡¨
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        print(f"âœ‚ï¸  æ­£åœ¨åˆ†å‰²æ–‡æœ¬ä¸ºç¿»è¯‘å—...")
        
        chunks = []
        current_chunk = ''
        current_line_count = 0
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å¤§å°æˆ–è¡Œæ•°é™åˆ¶
            if (len(current_chunk) + len(line + '\n') > self.chunk_size or 
                current_line_count >= self.max_chunk_lines):
                
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = line + '\n'
                current_line_count = 1
            else:
                current_chunk += line + '\n'
                current_line_count += 1
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        print(f"âœ… åˆ†å‰²ä¸º{len(chunks)}ä¸ªç¿»è¯‘å—")
        return chunks
    
    def get_chunk_context(self, chunks: List[str], chunk_index: int) -> Tuple[Optional[List[str]], Optional[List[str]]]:
        """
        è·å–ç¿»è¯‘å—çš„ä¸Šä¸‹æ–‡
        
        Args:
            chunks: æ‰€æœ‰å—åˆ—è¡¨
            chunk_index: å½“å‰å—ç´¢å¼•
            
        Returns:
            (å‰æ–‡è¡Œåˆ—è¡¨, åæ–‡è¡Œåˆ—è¡¨)
        """
        # è·å–å‰æ–‡ï¼ˆå‰ä¸€ä¸ªå—çš„æœ€å3è¡Œï¼‰
        previous_content = None
        if chunk_index > 0:
            prev_lines = chunks[chunk_index - 1].split('\n')
            previous_content = prev_lines[-3:] if len(prev_lines) >= 3 else prev_lines
        
        # è·å–åæ–‡ï¼ˆåä¸€ä¸ªå—çš„å‰2è¡Œï¼‰
        after_content = None
        if chunk_index < len(chunks) - 1:
            next_lines = chunks[chunk_index + 1].split('\n')
            after_content = next_lines[:2] if len(next_lines) >= 2 else next_lines
        
        return previous_content, after_content
    
    def search_relevant_terms(self, text: str) -> List[Dict]:
        """
        æœç´¢æ–‡æœ¬ä¸­çš„ç›¸å…³æœ¯è¯­
        
        Args:
            text: å¾…æœç´¢çš„æ–‡æœ¬
            
        Returns:
            ç›¸å…³æœ¯è¯­åˆ—è¡¨
        """
        relevant_terms = []
        text_lower = text.lower()
        
        for src_term, term_info in self._term_mapping.items():
            if src_term in text_lower:
                relevant_terms.append({
                    'src': src_term,
                    'tgt': term_info['tgt'],
                    'note': term_info['note'],
                    'category': term_info['category'],
                    'confidence': term_info['confidence']
                })
        
        # æŒ‰ç½®ä¿¡åº¦å’Œç±»åˆ«æ’åº
        relevant_terms.sort(key=lambda x: (x['category'] == 'custom', x['confidence']), reverse=True)
        
        return relevant_terms
    
    def generate_shared_prompt(self, 
                             previous_content: Optional[List[str]], 
                             after_content: Optional[List[str]], 
                             theme: str, 
                             relevant_terms: List[Dict]) -> str:
        """
        ç”Ÿæˆå…±äº«æç¤ºè¯
        
        Args:
            previous_content: å‰æ–‡å†…å®¹
            after_content: åæ–‡å†…å®¹  
            theme: å†…å®¹ä¸»é¢˜
            relevant_terms: ç›¸å…³æœ¯è¯­
            
        Returns:
            å…±äº«æç¤ºè¯
        """
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []
        
        if previous_content:
            context_parts.append(f"### Previous Content\n{chr(10).join(previous_content)}")
        
        if after_content:
            context_parts.append(f"### Subsequent Content\n{chr(10).join(after_content)}")
        
        # æ„å»ºæœ¯è¯­ä¿¡æ¯
        terms_info = ""
        if relevant_terms:
            terms_list = []
            for term in relevant_terms:
                terms_list.append(f"- {term['src']}: {term['tgt']} ({term['note']})")
            terms_info = f"### Key Terms\n{chr(10).join(terms_list)}"
        
        # ç»„åˆæç¤ºè¯
        shared_prompt = f"""### Context Information
{chr(10).join(context_parts)}

### Content Summary
{theme}

### Points to Note
{terms_info}"""
        
        return shared_prompt
    
    def get_faithful_translation_prompt(self, lines: str, shared_prompt: str) -> str:
        """ç”Ÿæˆå¿ å®ç¿»è¯‘æç¤ºè¯"""
        
        # æ„å»ºJSONæ ¼å¼æ¨¡æ¿
        line_splits = lines.split('\n')
        json_dict = {}
        for i, line in enumerate(line_splits, 1):
            json_dict[str(i)] = {
                "origin": line, 
                "direct": f"direct {self.tgt_language} translation {i}."
            }
        json_format = json.dumps(json_dict, indent=2, ensure_ascii=False)
        
        return f"""
## Role
You are a professional Netflix subtitle translator, fluent in both {self.src_language} and {self.tgt_language}, as well as their respective cultures. 
Your expertise lies in accurately understanding the semantics and structure of the original {self.src_language} text and faithfully translating it into {self.tgt_language} while preserving the original meaning.

## Task
We have a segment of original {self.src_language} subtitles that need to be directly translated into {self.tgt_language}. These subtitles come from a specific context and may contain specific themes and terminology.

1. Translate the original {self.src_language} subtitles into {self.tgt_language} line by line
2. Ensure the translation is faithful to the original, accurately conveying the original meaning
3. Consider the context and professional terminology

{shared_prompt}

<translation_principles>
1. Faithful to the original: Accurately convey the content and meaning of the original text, without arbitrarily changing, adding, or omitting content.
2. Accurate terminology: Use professional terms correctly and maintain consistency in terminology.
3. Understand the context: Fully comprehend and reflect the background and contextual relationships of the text.
</translation_principles>

## INPUT
<subtitles>
{lines}
</subtitles>

## Output in only JSON format and no other text
```json
{json_format}
```

Note: Start your answer with ```json and end with ```, do not add any other text.
""".strip()
    
    def get_expressive_translation_prompt(self, faithful_result: Dict, lines: str, shared_prompt: str) -> str:
        """ç”Ÿæˆè¡¨è¾¾ä¼˜åŒ–ç¿»è¯‘æç¤ºè¯"""
        
        # æ„å»ºJSONæ ¼å¼æ¨¡æ¿
        json_format = {}
        for key, value in faithful_result.items():
            json_format[key] = {
                "origin": value["origin"],
                "direct": value["direct"],
                "reflect": "your reflection on direct translation",
                "free": "your free translation"
            }
        json_format_str = json.dumps(json_format, indent=2, ensure_ascii=False)
        
        return f"""
## Role
You are a professional Netflix subtitle translator and language consultant.
Your expertise lies not only in accurately understanding the original {self.src_language} but also in optimizing the {self.tgt_language} translation to better suit the target language's expression habits and cultural background.

## Task
We already have a direct translation version of the original {self.src_language} subtitles.
Your task is to reflect on and improve these direct translations to create more natural and fluent {self.tgt_language} subtitles.

1. Analyze the direct translation results line by line, pointing out existing issues
2. Provide detailed modification suggestions
3. Perform free translation based on your analysis
4. Do not add comments or explanations in the translation, as the subtitles are for the audience to read
5. Do not leave empty lines in the free translation, as the subtitles are for the audience to read

{shared_prompt}

<Translation Analysis Steps>
Please use a two-step thinking process to handle the text line by line:

1. Direct Translation Reflection:
   - Evaluate language fluency
   - Check if the language style is consistent with the original text
   - Check the conciseness of the subtitles, point out where the translation is too wordy

2. {self.tgt_language} Free Translation:
   - Aim for contextual smoothness and naturalness, conforming to {self.tgt_language} expression habits
   - Ensure it's easy for {self.tgt_language} audience to understand and accept
   - Adapt the language style to match the theme (e.g., use casual language for tutorials, professional terminology for technical content, formal language for documentaries)
</Translation Analysis Steps>
   
## INPUT
<subtitles>
{lines}
</subtitles>

## Output in only JSON format and no other text
```json
{json_format_str}
```

Note: Start your answer with ```json and end with ```, do not add any other text.
""".strip()
    
    def validate_translation_result(self, result: Dict, required_keys: List[str], required_sub_keys: List[str]) -> Dict:
        """éªŒè¯ç¿»è¯‘ç»“æœæ ¼å¼"""
        # æ£€æŸ¥å¿…éœ€çš„é”®
        if not all(key in result for key in required_keys):
            missing_keys = set(required_keys) - set(result.keys())
            return {"status": "error", "message": f"Missing required key(s): {', '.join(missing_keys)}"}
        
        # æ£€æŸ¥æ‰€æœ‰é¡¹ç›®ä¸­çš„å¿…éœ€å­é”®
        for key in result:
            if not all(sub_key in result[key] for sub_key in required_sub_keys):
                missing_sub_keys = set(required_sub_keys) - set(result[key].keys())
                return {"status": "error", "message": f"Missing required sub-key(s) in item {key}: {', '.join(missing_sub_keys)}"}
        
        return {"status": "success", "message": "Translation completed"}
    
    def translate_single_chunk(self, 
                             chunk_text: str, 
                             chunk_index: int, 
                             chunks: List[str], 
                             theme: str) -> TranslationChunk:
        """
        ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—
        
        Args:
            chunk_text: æ–‡æœ¬å—å†…å®¹
            chunk_index: å—ç´¢å¼•
            chunks: æ‰€æœ‰å—åˆ—è¡¨
            theme: å†…å®¹ä¸»é¢˜
            
        Returns:
            ç¿»è¯‘ç»“æœ
        """
        import time
        start_time = time.time()
        
        if self._ask_gpt_func is None:
            # æ²¡æœ‰GPTå‡½æ•°æ—¶çš„ç®€å•å¤„ç†
            return TranslationChunk(
                index=chunk_index,
                source_text=chunk_text,
                translated_text=f"[éœ€è¦GPTç¿»è¯‘] {chunk_text}",
                confidence=0.0,
                processing_time=time.time() - start_time,
                error_message="GPT function not set"
            )
        
        try:
            # è·å–ä¸Šä¸‹æ–‡å’Œç›¸å…³æœ¯è¯­
            previous_content, after_content = self.get_chunk_context(chunks, chunk_index)
            relevant_terms = self.search_relevant_terms(chunk_text)
            
            # ç”Ÿæˆå…±äº«æç¤ºè¯
            shared_prompt = self.generate_shared_prompt(
                previous_content, after_content, theme, relevant_terms
            )
            
            # ç¬¬ä¸€é˜¶æ®µï¼šå¿ å®ç¿»è¯‘
            faithful_prompt = self.get_faithful_translation_prompt(chunk_text, shared_prompt)
            
            # é‡è¯•æœºåˆ¶
            line_count = len(chunk_text.split('\n'))
            for retry in range(3):
                try:
                    def valid_faithful(response_data):
                        return self.validate_translation_result(
                            response_data, 
                            [str(i) for i in range(1, line_count + 1)], 
                            ['direct']
                        )
                    
                    faithful_result = self._ask_gpt_func(
                        faithful_prompt + " " * retry,
                        resp_type='json',
                        valid_def=valid_faithful,
                        log_title='translate_faithful'
                    )
                    
                    if len(faithful_result) == line_count:
                        break
                        
                except Exception as e:
                    if retry == 2:
                        raise e
                    print(f"âš ï¸  å—{chunk_index}å¿ å®ç¿»è¯‘é‡è¯•{retry + 1}/3...")
            
            # æ¸…ç†æ¢è¡Œç¬¦
            for key in faithful_result:
                faithful_result[key]["direct"] = faithful_result[key]["direct"].replace('\n', ' ')
            
            # å¦‚æœä¸å¯ç”¨åæ€ç¿»è¯‘ï¼Œç›´æ¥è¿”å›å¿ å®ç¿»è¯‘
            if not self.enable_reflect_translate:
                final_translation = '\n'.join([
                    faithful_result[str(i)]["direct"].strip() 
                    for i in range(1, line_count + 1)
                ])
                
                return TranslationChunk(
                    index=chunk_index,
                    source_text=chunk_text,
                    translated_text=final_translation,
                    confidence=0.8,
                    processing_time=time.time() - start_time,
                    context_terms=[term['src'] for term in relevant_terms]
                )
            
            # ç¬¬äºŒé˜¶æ®µï¼šè¡¨è¾¾ä¼˜åŒ–
            expressive_prompt = self.get_expressive_translation_prompt(
                faithful_result, chunk_text, shared_prompt
            )
            
            for retry in range(3):
                try:
                    def valid_expressive(response_data):
                        return self.validate_translation_result(
                            response_data,
                            [str(i) for i in range(1, line_count + 1)],
                            ['free']
                        )
                    
                    expressive_result = self._ask_gpt_func(
                        expressive_prompt + " " * retry,
                        resp_type='json',
                        valid_def=valid_expressive,
                        log_title='translate_expressive'
                    )
                    
                    if len(expressive_result) == line_count:
                        break
                        
                except Exception as e:
                    if retry == 2:
                        raise e
                    print(f"âš ï¸  å—{chunk_index}è¡¨è¾¾ä¼˜åŒ–é‡è¯•{retry + 1}/3...")
            
            # ç»„åˆæœ€ç»ˆç¿»è¯‘ç»“æœ
            final_translation = '\n'.join([
                expressive_result[str(i)]["free"].replace('\n', ' ').strip()
                for i in range(1, line_count + 1)
            ])
            
            # éªŒè¯ç¿»è¯‘ç»“æœé•¿åº¦
            if len(chunk_text.split('\n')) != len(final_translation.split('\n')):
                raise ValueError(f"Translation length mismatch for chunk {chunk_index}")
            
            return TranslationChunk(
                index=chunk_index,
                source_text=chunk_text,
                translated_text=final_translation,
                confidence=0.9,
                processing_time=time.time() - start_time,
                context_terms=[term['src'] for term in relevant_terms]
            )
            
        except Exception as e:
            return TranslationChunk(
                index=chunk_index,
                source_text=chunk_text,
                translated_text="",
                confidence=0.0,
                processing_time=time.time() - start_time,
                retry_count=3,
                error_message=str(e)
            )
    
    def translate_all_chunks(self, chunks: List[str], theme: str) -> List[TranslationChunk]:
        """
        å¹¶è¡Œç¿»è¯‘æ‰€æœ‰æ–‡æœ¬å—
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            theme: å†…å®¹ä¸»é¢˜
            
        Returns:
            ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œç¿»è¯‘{len(chunks)}ä¸ªæ–‡æœ¬å—...")
        
        results = [None] * len(chunks)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
            futures = []
            for i, chunk in enumerate(chunks):
                future = executor.submit(
                    self.translate_single_chunk, 
                    chunk, i, chunks, theme
                )
                futures.append((future, i))
            
            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future, index in futures:
                try:
                    result = future.result()
                    results[index] = result
                    completed_count += 1
                    
                    if result.error_message:
                        print(f"âŒ å—{index}ç¿»è¯‘å¤±è´¥: {result.error_message}")
                    else:
                        print(f"âœ… å—{index}ç¿»è¯‘å®Œæˆ (è€—æ—¶{result.processing_time:.2f}s)")
                        
                except Exception as e:
                    print(f"ğŸ’¥ å—{index}å¤„ç†å¼‚å¸¸: {str(e)}")
                    results[index] = TranslationChunk(
                        index=index,
                        source_text=chunks[index] if index < len(chunks) else "",
                        error_message=str(e)
                    )
        
        print(f"ğŸ‰ å¹¶è¡Œç¿»è¯‘å®Œæˆ: {completed_count}/{len(chunks)} æˆåŠŸ")
        return [r for r in results if r is not None]
    
    def similarity_match_results(self, chunks: List[str], results: List[TranslationChunk]) -> List[str]:
        """
        ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…ç¿»è¯‘ç»“æœ
        
        Args:
            chunks: åŸå§‹æ–‡æœ¬å—
            results: ç¿»è¯‘ç»“æœ
            
        Returns:
            åŒ¹é…åçš„ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨
        """
        print("ğŸ” æ­£åœ¨è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…...")
        
        matched_translations = []
        
        for i, chunk in enumerate(chunks):
            chunk_lines = chunk.split('\n')
            chunk_text = ''.join(chunk_lines).lower()
            
            # è®¡ç®—ä¸æ‰€æœ‰ç»“æœçš„ç›¸ä¼¼åº¦
            similarities = []
            for result in results:
                if result.source_text:
                    source_text = ''.join(result.source_text.split('\n')).lower()
                    similarity = SequenceMatcher(None, chunk_text, source_text).ratio()
                    similarities.append((result, similarity))
            
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…
            if similarities:
                best_match, best_similarity = max(similarities, key=lambda x: x[1])
                
                if best_similarity < self.similarity_threshold:
                    print(f"âš ï¸  å—{i}åŒ¹é…åº¦è¾ƒä½: {best_similarity:.3f}")
                    if best_similarity < 0.8:
                        raise ValueError(f"Translation matching failed for chunk {i}")
                
                if best_match.translated_text:
                    matched_translations.extend(best_match.translated_text.split('\n'))
                else:
                    # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡
                    matched_translations.extend(chunk_lines)
            else:
                matched_translations.extend(chunk_lines)
        
        print(f"âœ… ç›¸ä¼¼åº¦åŒ¹é…å®Œæˆï¼Œå…±{len(matched_translations)}è¡Œ")
        return matched_translations
    
    def save_translation_results(self, source_lines: List[str], translated_lines: List[str]) -> TranslationResult:
        """
        ä¿å­˜ç¿»è¯‘ç»“æœ
        
        Args:
            source_lines: æºæ–‡æœ¬è¡Œ
            translated_lines: ç¿»è¯‘æ–‡æœ¬è¡Œ
            
        Returns:
            ç¿»è¯‘ç»“æœç»Ÿè®¡
        """
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç¿»è¯‘ç»“æœ...")
        
        try:
            pd = self._get_pandas()
            
            # åˆ›å»ºåŒè¯­å¯¹ç…§æ•°æ®æ¡†
            df_translate = pd.DataFrame({
                'Source': source_lines,
                'Translation': translated_lines
            })
            
            # ä¿å­˜ä¸ºExcelæ–‡ä»¶
            df_translate.to_excel(self.translation_file, index=False)
            
            print(f"âœ… ç¿»è¯‘ç»“æœå·²ä¿å­˜: {self.translation_file}")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {len(source_lines)}è¡Œæºæ–‡æœ¬ï¼Œ{len(translated_lines)}è¡Œç¿»è¯‘")
            
            return TranslationResult(
                total_chunks=len(source_lines),
                successful_chunks=len(translated_lines),
                failed_chunks=max(0, len(source_lines) - len(translated_lines)),
                total_time=0.0,
                average_confidence=0.85,
                output_file=str(self.translation_file)
            )
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¿»è¯‘ç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def process_complete_translation(self) -> TranslationResult:
        """
        å®Œæ•´çš„æ–‡æœ¬ç¿»è¯‘å¤„ç†æµç¨‹
        
        Returns:
            ç¿»è¯‘ç»“æœ
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´æ–‡æœ¬ç¿»è¯‘æµç¨‹...")
        
        try:
            # 1. åŠ è½½ç¿»è¯‘ä¸Šä¸‹æ–‡
            terminology_data, translation_context, content_theme = self.load_translation_context()
            
            # 2. åŠ è½½è¾“å…¥æ–‡æœ¬
            text_lines = self.load_input_text()
            
            # 3. åˆ†å‰²ä¸ºç¿»è¯‘å—
            chunks = self.split_text_into_chunks(text_lines)
            
            # 4. å¹¶è¡Œç¿»è¯‘æ‰€æœ‰å—
            translation_results = self.translate_all_chunks(chunks, content_theme)
            
            # 5. ç›¸ä¼¼åº¦åŒ¹é…ç»“æœ
            matched_translations = self.similarity_match_results(chunks, translation_results)
            
            # 6. é‡å»ºæºæ–‡æœ¬è¡Œåˆ—è¡¨
            source_lines = []
            for chunk in chunks:
                source_lines.extend(chunk.split('\n'))
            
            # 7. ä¿å­˜ç¿»è¯‘ç»“æœ
            result = self.save_translation_results(source_lines, matched_translations)
            
            print("ğŸ‰ æ–‡æœ¬ç¿»è¯‘æµç¨‹å®Œæˆï¼")
            print(f"ğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
            print(f"  ğŸ“„ æ€»å—æ•°: {len(chunks)}")
            print(f"  âœ… æˆåŠŸ: {result.successful_chunks}")
            print(f"  âŒ å¤±è´¥: {result.failed_chunks}")
            print(f"  ğŸ“ æœ¯è¯­ä½¿ç”¨: {len(self._term_mapping)}ä¸ª")
            print(f"  ğŸ“ è¾“å‡ºæ–‡ä»¶: {result.output_file}")
            
            return result
            
        except Exception as e:
            print(f"ğŸ’¥ æ–‡æœ¬ç¿»è¯‘æµç¨‹å¤±è´¥: {str(e)}")
            raise


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºç¿»è¯‘å™¨å®ä¾‹
    translator = TextTranslator(
        src_language='en',
        tgt_language='zh',
        chunk_size=600,
        max_chunk_lines=10,
        max_workers=2,
        enable_reflect_translate=True
    )
    
    # æµ‹è¯•å‚æ•°
    test_with_gpt = '--gpt' in sys.argv
    
    if test_with_gpt:
        print("âš ï¸  æ³¨æ„: éœ€è¦æä¾›GPTå‡½æ•°æ‰èƒ½è¿›è¡Œå®Œæ•´æµ‹è¯•")
        # translator.set_gpt_function(your_gpt_function)
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not translator.input_file.exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {translator.input_file}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ–‡æœ¬åˆ†å‰²å™¨ç”Ÿæˆåˆ†å‰²æ–‡ä»¶")
            sys.exit(1)
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡æ–‡ä»¶
        if not translator.terminology_file.exists():
            print(f"âŒ æœ¯è¯­æ–‡ä»¶ä¸å­˜åœ¨: {translator.terminology_file}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œå†…å®¹æ€»ç»“å™¨ç”Ÿæˆæœ¯è¯­æ–‡ä»¶")
            sys.exit(1)
        
        # è¿è¡Œå®Œæ•´ç¿»è¯‘æµç¨‹
        print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬ç¿»è¯‘æµç¨‹...")
        
        result = translator.process_complete_translation()
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result.output_file}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        if Path(result.output_file).exists():
            pd = translator._get_pandas()
            df = pd.read_excel(result.output_file)
            
            print(f"\nğŸ“‹ ç¿»è¯‘é¢„è§ˆ (å‰5è¡Œ):")
            for i in range(min(5, len(df))):
                source = df.iloc[i]['Source']
                translation = df.iloc[i]['Translation']
                print(f"  {i+1:2d}. {source[:50]}...")
                print(f"      â†’ {translation[:50]}...")
                print()
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1) 