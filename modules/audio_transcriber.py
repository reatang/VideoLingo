"""
# ----------------------------------------------------------------------------
# éŸ³é¢‘è½¬å½•å™¨æ¨¡å— - è´Ÿè´£å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬
# 
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. è§†é¢‘åˆ°éŸ³é¢‘çš„è½¬æ¢
# 2. éŸ³é¢‘éŸ³é‡æ ‡å‡†åŒ–
# 3. éŸ³é¢‘æ™ºèƒ½åˆ†æ®µ
# 4. å¤šç§ASRå¼•æ“æ”¯æŒ (WhisperXæœ¬åœ°/äº‘ç«¯, ElevenLabs)
# 5. äººå£°åˆ†ç¦» (å¯é€‰)
# 
# è¾“å…¥ï¼šè§†é¢‘æ–‡ä»¶è·¯å¾„
# è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„è½¬å½•ç»“æœDataFrameå’ŒExcelæ–‡ä»¶
# ----------------------------------------------------------------------------
"""

import os
import pandas as pd
from typing import Dict, List, Tuple, Optional, Callable
from pathlib import Path

from modules.asr_backend.base import ASRResult
from modules.asr_backend.utils import AudioProcessor
from modules.asr_backend.factory import create_asr_engine, cleanup_all_engines


class AudioTranscriber:
    """éŸ³é¢‘è½¬å½•å™¨ç±» - å°è£…éŸ³é¢‘è½¬å½•çš„æ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self,
                 output_dir: str = 'output',
                 audio_dir: str = 'output/audio', 
                 target_segment_length: float = 30*60,
                 silence_window: float = 60,
                 target_db: float = -20.0):
        """
        åˆå§‹åŒ–éŸ³é¢‘è½¬å½•å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            audio_dir: éŸ³é¢‘æ–‡ä»¶ç›®å½•
            target_segment_length: ç›®æ ‡åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
            silence_window: é™é»˜æ£€æµ‹çª—å£ï¼ˆç§’ï¼‰
            target_db: ç›®æ ‡éŸ³é‡æ ‡å‡†åŒ–dBå€¼
        """
        self.output_dir = Path(output_dir)
        self.audio_dir = Path(audio_dir)
        self.target_segment_length = target_segment_length
        self.silence_window = silence_window
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.audio_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'log').mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨ - ä½¿ç”¨å·²æœ‰çš„AudioProcessorç±»
        self.audio_processor = AudioProcessor(
            target_db=target_db,
            safe_margin=0.5
        )
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.raw_audio_file = self.audio_dir / 'raw_audio.mp3'
        self.vocal_audio_file = self.audio_dir / 'vocal_audio.mp3'
        self.cleaned_chunks_file = self.output_dir / 'log' / '2_cleaned_chunks.xlsx'
    
    def convert_video_to_audio(self, video_file: str) -> str:
        """
        å°†è§†é¢‘æ–‡ä»¶è½¬æ¢ä¸ºéŸ³é¢‘æ–‡ä»¶
        
        Args:
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(video_file):
            raise FileNotFoundError(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_file}")
        
        if self.raw_audio_file.exists():
            print(f"âœ… éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {self.raw_audio_file}")
            return str(self.raw_audio_file)
        
        # ä½¿ç”¨AudioProcessorè¿›è¡Œè½¬æ¢
        return self.audio_processor.convert_video_to_audio(
            video_file,
            str(self.raw_audio_file),
            audio_format="mp3",
            sample_rate=16000,
            channels=1,
            bitrate="32k"
        )
    
    def normalize_audio_volume(self, 
                             audio_path: str, 
                             output_path: Optional[str] = None,
                             format: str = "mp3") -> str:
        """
        æ ‡å‡†åŒ–éŸ³é¢‘éŸ³é‡
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„, å¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶
            format: è¾“å‡ºæ ¼å¼
            
        Returns:
            æ ‡å‡†åŒ–åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨AudioProcessorçš„æ ‡å‡†åŒ–åŠŸèƒ½
        return self.audio_processor.normalize_audio_volume(
            audio_path, output_path or audio_path, format=format
        )
    
    def get_audio_duration(self, audio_file: str) -> float:
        """
        è·å–éŸ³é¢‘æ–‡ä»¶æ—¶é•¿
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        # ä½¿ç”¨AudioProcessorè·å–æ—¶é•¿
        return self.audio_processor.get_audio_duration(audio_file)
    
    def split_audio_by_silence(self, audio_file: str) -> List[Tuple[float, float]]:
        """
        åŸºäºé™é»˜æ£€æµ‹æ™ºèƒ½åˆ†å‰²éŸ³é¢‘
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æ®µåˆ—è¡¨, æ¯ä¸ªå…ƒç´ ä¸º(å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´)çš„å…ƒç»„
        """
        # ä½¿ç”¨AudioProcessorè¿›è¡Œåˆ†æ®µ
        return self.audio_processor.split_audio_by_silence(
            audio_file,
            target_length=self.target_segment_length,
            silence_window=self.silence_window
        )
    
    def process_transcription_result(self, result: ASRResult) -> pd.DataFrame:
        """
        å¤„ç†è½¬å½•ç»“æœ, è½¬æ¢ä¸ºæ ‡å‡†DataFrameæ ¼å¼
        
        Args:
            result: ASRå¼•æ“è¿”å›çš„è½¬å½•ç»“æœ
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        # ä½¿ç”¨AudioProcessorå¤„ç†è½¬å½•ç»“æœ
        return self.audio_processor.process_transcription_result(result.to_dict())
    
    def save_transcription_results(self, df: pd.DataFrame) -> str:
        """
        ä¿å­˜è½¬å½•ç»“æœåˆ°Excelæ–‡ä»¶
        
        Args:
            df: è½¬å½•ç»“æœDataFrame
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨AudioProcessorä¿å­˜ç»“æœ
        return self.audio_processor.save_transcription_results(
            df, str(self.cleaned_chunks_file)
        )
    
    def transcribe_audio_segment(self, 
                               audio_file: str,
                               vocal_audio_file: Optional[str] = None,
                               start_time: float = 0,
                               end_time: Optional[float] = None,
                               engine_type: str = "local",
                               config: Optional[Dict] = None) -> ASRResult:
        """
        è½¬å½•éŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            vocal_audio_file: äººå£°åˆ†ç¦»åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            engine_type: ASRå¼•æ“ç±»å‹
            config: å¼•æ“é…ç½®
            
        Returns:
            è½¬å½•ç»“æœ
        """
        print(f"ğŸ¤ è½¬å½•éŸ³é¢‘ç‰‡æ®µ: {start_time:.1f}s - {end_time or 'ç»“æŸ'}s")
        
        # åˆ›å»ºASRå¼•æ“
        asr_engine = create_asr_engine(engine_type, config)
        
        try:
            # æ‰§è¡Œè½¬å½•
            result = asr_engine.transcribe(
                raw_audio_path=audio_file,
                vocal_audio_path=vocal_audio_file or audio_file,
                start_time=start_time,
                end_time=end_time
            )
            
            return result
            
        finally:
            # æ¸…ç†å¼•æ“èµ„æº
            asr_engine.cleanup()
    
    def transcribe_video_complete(self, 
                                video_file: str,
                                use_vocal_separation: bool = False,
                                engine_type: str = "local",
                                config: Optional[Dict] = None) -> str:
        """
        å®Œæ•´çš„è§†é¢‘è½¬å½•æµç¨‹
        
        Args:
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            use_vocal_separation: æ˜¯å¦ä½¿ç”¨äººå£°åˆ†ç¦»
            engine_type: ASRå¼•æ“ç±»å‹
            config: å¼•æ“é…ç½®
            
        Returns:
            ä¿å­˜çš„è½¬å½•ç»“æœæ–‡ä»¶è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´è§†é¢‘è½¬å½•æµç¨‹...")
        
        try:
            # 1. è§†é¢‘è½¬éŸ³é¢‘
            audio_file = self.convert_video_to_audio(video_file)
            
            # 2. äººå£°åˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
            if use_vocal_separation:
                print("ğŸµ æ­£åœ¨è¿›è¡Œäººå£°åˆ†ç¦»...")
                vocal_audio = str(self.vocal_audio_file)
                # è¿™é‡Œéœ€è¦å¤–éƒ¨æä¾›äººå£°åˆ†ç¦»å‡½æ•°
                # vocal_separation_func(audio_file, vocal_audio)
            else:
                vocal_audio = audio_file
            
            # 3. éŸ³é¢‘åˆ†æ®µ
            segments = self.split_audio_by_silence(audio_file)
            
            # 4. åˆ†æ®µè½¬å½•
            all_results = []
            for i, (start, end) in enumerate(segments):
                print(f"ğŸ¤ è½¬å½•ç¬¬{i+1}/{len(segments)}ä¸ªç‰‡æ®µ...")
                result = self.transcribe_audio_segment(
                    audio_file, vocal_audio, start, end, engine_type, config
                )
                all_results.append(result)
            
            # 5. åˆå¹¶ç»“æœ
            combined_words = []
            for result in all_results:
                result_df = result.to_dataframe()
                if not result_df.empty:
                    combined_words.append(result_df)
            
            if not combined_words:
                raise ValueError("âŒ æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„è½¬å½•ç»“æœ")
            
            combined_df = pd.concat(combined_words, ignore_index=True)
            
            # 6. ä¿å­˜ç»“æœ
            output_file = self.save_transcription_results(combined_df)
            
            print("ğŸ‰ è§†é¢‘è½¬å½•æµç¨‹å®Œæˆï¼")
            return output_file
            
        except Exception as e:
            print(f"ğŸ’¥ è½¬å½•æµç¨‹å¤±è´¥: {str(e)}")
            raise
        finally:
            # ç¡®ä¿æ¸…ç†æ‰€æœ‰å¼•æ“èµ„æº
            cleanup_all_engines()


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºè½¬å½•å™¨å®ä¾‹
    transcriber = AudioTranscriber()
    
    # æµ‹è¯•è§†é¢‘æ–‡ä»¶è¾“å…¥
    if len(sys.argv) > 1:
        video_file = sys.argv[1]
    else:
        video_file = input('è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„: ')
    
    if not video_file.strip():
        print("âŒ è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        sys.exit(1)
    
    try:
        # æµ‹è¯•å®Œæ•´è½¬å½•æµç¨‹
        print("\nğŸ§ª æµ‹è¯•å®Œæ•´è½¬å½•æµç¨‹...")
        output_file = transcriber.transcribe_video_complete(
            video_file,
            engine_type="local"
        )
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ è½¬å½•ç»“æœ: {output_file}")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1) 