"""
# ----------------------------------------------------------------------------
# éŸ³é¢‘è½¬å½•å™¨æ¨¡å— - è´Ÿè´£å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬
# 
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. è§†é¢‘åˆ°éŸ³é¢‘çš„è½¬æ¢
# 2. éŸ³é¢‘éŸ³é‡æ ‡å‡†åŒ–
# 3. éŸ³é¢‘æ™ºèƒ½åˆ†æ®µ
# 4. å¤šç§ASRå¼•æ“æ”¯æŒ (WhisperXæœ¬åœ°/äº‘ç«¯, ElevenLabs)
# 5. äººå£°åˆ†ç¦» (å¯é€‰)
# 
# è¾“å…¥ï¼šè§†é¢‘æ–‡ä»¶è·¯å¾„
# è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„è½¬å½•ç»“æœDataFrameå’ŒExcelæ–‡ä»¶
# ----------------------------------------------------------------------------
"""

import os
import pandas as pd
from typing import Dict, List, Tuple, Optional, Callable
from pathlib import Path

from modules.asr_backend.base import ASRResult
from modules.asr_backend.utils import AudioProcessor
from modules.asr_backend.factory import create_asr_engine, cleanup_all_engines
from modules.common_utils import paths


class AudioTranscriber:
    """éŸ³é¢‘è½¬å½•å™¨ç±» - å°è£…éŸ³é¢‘è½¬å½•çš„æ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self,
                 output_dir: str = 'output',
                 target_segment_length: float = 30*60,
                 silence_window: float = 60,
                 target_db: float = -20.0):
        """
        åˆå§‹åŒ–éŸ³é¢‘è½¬å½•å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            target_segment_length: ç›®æ ‡åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
            silence_window: é™é»˜æ£€æµ‹çª—å£ï¼ˆç§’ï¼‰
            target_db: ç›®æ ‡éŸ³é‡æ ‡å‡†åŒ–dBå€¼
        """
        self.output_dir = Path(output_dir)
        self.audio_dir = self.output_dir / "audio"
        self.target_segment_length = target_segment_length
        self.silence_window = silence_window
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.audio_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'log').mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨ 
        self.target_db = target_db
        self.safe_margin = 0.5
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.raw_audio_file = self.audio_dir / 'raw.mp3'
        self.vocal_audio_file = self.audio_dir / 'vocal.mp3'
    
    def _convert_video_to_audio(self, video_file: str) -> str:
        """
        å°†è§†é¢‘æ–‡ä»¶è½¬æ¢ä¸ºéŸ³é¢‘æ–‡ä»¶
        
        Args:
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(video_file):
            raise FileNotFoundError(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_file}")
        
        if self.raw_audio_file.exists():
            print(f"âœ… éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {self.raw_audio_file}")
            return str(self.raw_audio_file)
        
        # ä½¿ç”¨AudioProcessorè¿›è¡Œè½¬æ¢
        return AudioProcessor.convert_video_to_audio(
            video_file,
            str(self.raw_audio_file),
            audio_format="mp3",
            sample_rate=16000,
            channels=1,
            bitrate="32k"
        )
    
    def _merge_transcription_results(self, results: List[ASRResult]) -> pd.DataFrame:
        """
        åˆå¹¶å¤šä¸ªè½¬å½•ç»“æœ
        
        Args:
            results: ASRè½¬å½•ç»“æœåˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„DataFrame
        """
        print("ğŸ”— æ­£åœ¨åˆå¹¶å¤šä¸ªè½¬å½•ç»“æœ...")
        
        all_dfs = []
        for i, result in enumerate(results):
            print(f"ğŸ“ å¤„ç†ç¬¬{i+1}/{len(results)}ä¸ªè½¬å½•ç»“æœ...")
            df = AudioProcessor.process_transcription_result(result)
            all_dfs.append(df)
        
        # åˆå¹¶æ‰€æœ‰DataFrame
        if all_dfs:
            combined_df = pd.concat(all_dfs, ignore_index=True)
            print(f"âœ… è½¬å½•ç»“æœåˆå¹¶å®Œæˆï¼Œå…±{len(combined_df)}ä¸ªè¯æ±‡")
            return combined_df
        else:
            raise ValueError("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è½¬å½•ç»“æœå¯åˆå¹¶")
    
    def _save_transcription_results(self, df: pd.DataFrame, output_xlsx_file: str) -> str:
        """
        ä¿å­˜è½¬å½•ç»“æœåˆ°Excelæ–‡ä»¶
        
        Args:
            df: è½¬å½•ç»“æœDataFrame
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜è½¬å½•ç»“æœ...")
        
        # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = output_xlsx_file
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # ä¸ºæ–‡æœ¬æ·»åŠ å¼•å·ï¼ˆExcelæ ¼å¼è¦æ±‚ï¼‰
        df_copy = df.copy()
        df_copy['text'] = df_copy['text'].apply(lambda x: f'"{x}"')
        
        # ä¿å­˜åˆ°Excel
        try:
            df_copy.to_excel(output_file, index=False)
            print(f"âœ… è½¬å½•ç»“æœå·²ä¿å­˜: {output_file}")
            print(f"ğŸ“ˆ æœ€ç»ˆæ•°æ®ç»Ÿè®¡: {len(df_copy)}è¡Œè®°å½•")
            
            return output_file
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è½¬å½•ç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def transcribe_audio_segment(self, 
                               audio_file: str,
                               vocal_audio_file: Optional[str] = None,
                               start_time: float = 0,
                               end_time: Optional[float] = None,
                               engine_type: str = "local",
                               config: Optional[Dict] = None) -> ASRResult:
        """
        è½¬å½•éŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            vocal_audio_file: äººå£°åˆ†ç¦»åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            engine_type: ASRå¼•æ“ç±»å‹
            config: å¼•æ“é…ç½®
            
        Returns:
            è½¬å½•ç»“æœ
        """
        print(f"ğŸ¤ è½¬å½•éŸ³é¢‘ç‰‡æ®µ: {start_time:.1f}s - {end_time or 'ç»“æŸ'}s")
        
        # åˆ›å»ºASRå¼•æ“
        asr_engine = create_asr_engine(engine_type, config)
        
        try:
            # æ‰§è¡Œè½¬å½•
            result = asr_engine.transcribe(
                raw_audio_path=audio_file,
                vocal_audio_path=vocal_audio_file or audio_file,
                start_time=start_time,
                end_time=end_time
            )
            
            return result
            
        finally:
            # æ¸…ç†å¼•æ“èµ„æº
            asr_engine.cleanup()
    
    def transcribe_video_complete(self, 
                                video_file: str,
                                output_xlsx_file: Optional[str] = None,
                                use_vocal_separation: bool = False,
                                engine_type: str = "local",
                                config: Optional[Dict] = None) -> str:
        """
        å®Œæ•´çš„è§†é¢‘è½¬å½•æµç¨‹
        
        Args:
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_xlsx_file: è¾“å‡ºExcelæ–‡ä»¶å
            use_vocal_separation: æ˜¯å¦ä½¿ç”¨äººå£°åˆ†ç¦»
            engine_type: ASRå¼•æ“ç±»å‹
            config: å¼•æ“é…ç½®
            
        Returns:
            ä¿å­˜çš„è½¬å½•ç»“æœæ–‡ä»¶è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´è§†é¢‘è½¬å½•æµç¨‹...")

        if output_xlsx_file is None:
            output_xlsx_file = paths.get_filepath_by_log_dir('cleaned_chunks.xlsx', output_base_dir=self.output_dir)
        else:
            output_xlsx_file = paths.get_filepath_by_default(output_xlsx_file, output_base_dir=self.output_dir)
        
        try:
            # 1. è§†é¢‘è½¬éŸ³é¢‘
            audio_file = self._convert_video_to_audio(video_file)
            
            # 2. äººå£°åˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
            if use_vocal_separation:
                print("ğŸµ æ­£åœ¨è¿›è¡Œäººå£°åˆ†ç¦»...")
                vocal_audio = str(self.vocal_audio_file)
                # è¿™é‡Œéœ€è¦å¤–éƒ¨æä¾›äººå£°åˆ†ç¦»å‡½æ•°ï¼Œåˆ†ç¦»åè¿›è¡ŒéŸ³é‡æ ‡å‡†åŒ–
                # å‡è®¾äººå£°åˆ†ç¦»å·²å®Œæˆï¼Œå¯¹åˆ†ç¦»åçš„éŸ³é¢‘è¿›è¡Œæ ‡å‡†åŒ–
                AudioProcessor.normalize_audio_volume(audio_file, vocal_audio, target_db=self.target_db, format="mp3")
            else:
                vocal_audio = audio_file
            
            # 3. éŸ³é¢‘åˆ†æ®µ
            segments = AudioProcessor.split_audio_by_silence(
                audio_file,
                target_length=self.target_segment_length,
                silence_window=self.silence_window
            )
            
            # 4. åˆ†æ®µè½¬å½•
            all_results = []
            for i, (start, end) in enumerate(segments):
                print(f"ğŸ¤ è½¬å½•ç¬¬{i+1}/{len(segments)}ä¸ªç‰‡æ®µ...")
                result = self.transcribe_audio_segment(
                    audio_file, vocal_audio, start, end, engine_type, config
                )
                all_results.append(result)
            
            # 5. åˆå¹¶ç»“æœ
            combined_df = self._merge_transcription_results(all_results)

            # 6. ä¿å­˜ç»“æœ
            output_file = self._save_transcription_results(combined_df, output_xlsx_file)
            
            print("ğŸ‰ è§†é¢‘è½¬å½•æµç¨‹å®Œæˆï¼")
            return output_file
            
        except Exception as e:
            print(f"ğŸ’¥ è½¬å½•æµç¨‹å¤±è´¥: {str(e)}")
            raise
        finally:
            # ç¡®ä¿æ¸…ç†æ‰€æœ‰å¼•æ“èµ„æº
            cleanup_all_engines()


def transcribe_video_complete(video_file: str,
                              output_dir: str = "output",
                             output_xlsx_file: Optional[str] = None,
                             use_vocal_separation: bool = False,
                             engine_type: str = "local",
                             config: Optional[Dict] = None) -> str:
    """
    ä¸€é”®å®Œæˆè§†é¢‘è½¬å½•çš„ä¾¿æ·æ¥å£
    """
    transcriber = AudioTranscriber(output_dir=output_dir)
    return transcriber.transcribe_video_complete(video_file, output_xlsx_file, use_vocal_separation, engine_type, config)

# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºè½¬å½•å™¨å®ä¾‹
    transcriber = AudioTranscriber()
    
    # æµ‹è¯•è§†é¢‘æ–‡ä»¶è¾“å…¥
    if len(sys.argv) > 1:
        video_file = sys.argv[1]
    else:
        video_file = input('è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„: ')
    
    if not video_file.strip():
        print("âŒ è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        sys.exit(1)
    
    try:
        # æµ‹è¯•å®Œæ•´è½¬å½•æµç¨‹
        print("\nğŸ§ª æµ‹è¯•å®Œæ•´è½¬å½•æµç¨‹...")
        output_file = transcriber.transcribe_video_complete(
            video_file,
            engine_type="local"
        )
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ è½¬å½•ç»“æœ: {output_file}")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1) 