"""
# ----------------------------------------------------------------------------
# éŸ³é¢‘è½¬å½•å™¨æ¨¡å— - è´Ÿè´£å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬
# 
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. è§†é¢‘åˆ°éŸ³é¢‘çš„è½¬æ¢
# 2. éŸ³é¢‘éŸ³é‡æ ‡å‡†åŒ–
# 3. éŸ³é¢‘æ™ºèƒ½åˆ†æ®µ
# 4. å¤šç§ASRå¼•æ“æ”¯æŒ (WhisperXæœ¬åœ°/äº‘ç«¯, ElevenLabs)
# 5. äººå£°åˆ†ç¦» (å¯é€‰)
# 
# è¾“å…¥ï¼šè§†é¢‘æ–‡ä»¶è·¯å¾„
# è¾“å‡ºï¼šæ ‡å‡†åŒ–çš„è½¬å½•ç»“æœDataFrameå’ŒExcelæ–‡ä»¶
# ----------------------------------------------------------------------------
"""

import os
import subprocess
import pandas as pd
from typing import Dict, List, Tuple, Optional, Callable
from pydub import AudioSegment
from pydub.silence import detect_silence
from pydub.utils import mediainfo
from pathlib import Path


class AudioTranscriber:
    """éŸ³é¢‘è½¬å½•å™¨ç±» - å°è£…éŸ³é¢‘è½¬å½•çš„æ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self,
                 output_dir: str = 'output',
                 audio_dir: str = 'output/audio',
                 target_segment_length: float = 30*60,
                 silence_window: float = 60,
                 target_db: float = -20.0):
        """
        åˆå§‹åŒ–éŸ³é¢‘è½¬å½•å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            audio_dir: éŸ³é¢‘æ–‡ä»¶ç›®å½•
            target_segment_length: ç›®æ ‡åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
            silence_window: é™é»˜æ£€æµ‹çª—å£ï¼ˆç§’ï¼‰
            target_db: ç›®æ ‡éŸ³é‡æ ‡å‡†åŒ–dBå€¼
        """
        self.output_dir = Path(output_dir)
        self.audio_dir = Path(audio_dir)
        self.target_segment_length = target_segment_length
        self.silence_window = silence_window
        self.target_db = target_db
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        self.audio_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'log').mkdir(exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.raw_audio_file = self.audio_dir / 'raw_audio.mp3'
        self.vocal_audio_file = self.audio_dir / 'vocal_audio.mp3'
        self.cleaned_chunks_file = self.output_dir / 'log' / '2_cleaned_chunks.xlsx'
    
    def convert_video_to_audio(self, video_file: str) -> str:
        """
        å°†è§†é¢‘æ–‡ä»¶è½¬æ¢ä¸ºéŸ³é¢‘æ–‡ä»¶
        
        Args:
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Note: ä¿®å¤åŸä»£ç ç¼ºé™· - å¢åŠ äº†æ–‡ä»¶å­˜åœ¨æ£€æŸ¥å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†
        """
        if not os.path.exists(video_file):
            raise FileNotFoundError(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_file}")
        
        if self.raw_audio_file.exists():
            print(f"âœ… éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {self.raw_audio_file}")
            return str(self.raw_audio_file)
        
        print(f"ğŸ¬â¡ï¸ğŸµ æ­£åœ¨ä½¿ç”¨FFmpegè½¬æ¢ä¸ºé«˜è´¨é‡éŸ³é¢‘...")
        
        try:
            # ä½¿ç”¨FFmpegè½¬æ¢è§†é¢‘ä¸ºéŸ³é¢‘
            cmd = [
                'ffmpeg', '-y', '-i', video_file, '-vn',
                '-c:a', 'libmp3lame', '-b:a', '32k',
                '-ar', '16000', '-ac', '1', 
                '-metadata', 'encoding=UTF-8', 
                str(self.raw_audio_file)
            ]
            
            result = subprocess.run(
                cmd, 
                check=True, 
                capture_output=True, 
                text=True
            )
            
            print(f"âœ… è§†é¢‘è½¬éŸ³é¢‘å®Œæˆ: {video_file} -> {self.raw_audio_file}")
            return str(self.raw_audio_file)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ è§†é¢‘è½¬éŸ³é¢‘å¤±è´¥: {e.stderr}")
            raise RuntimeError(f"FFmpegè½¬æ¢å¤±è´¥: {e.stderr}")
        except FileNotFoundError:
            raise RuntimeError("âŒ æœªæ‰¾åˆ°FFmpeg, è¯·ç¡®ä¿å·²å®‰è£…å¹¶æ·»åŠ åˆ°PATHç¯å¢ƒå˜é‡")
    
    def normalize_audio_volume(self, 
                             audio_path: str, 
                             output_path: Optional[str] = None,
                             format: str = "mp3") -> str:
        """
        æ ‡å‡†åŒ–éŸ³é¢‘éŸ³é‡
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„, å¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶
            format: è¾“å‡ºæ ¼å¼
            
        Returns:
            æ ‡å‡†åŒ–åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        if output_path is None:
            output_path = audio_path
            
        try:
            audio = AudioSegment.from_file(audio_path)
            original_db = audio.dBFS
            
            # è®¡ç®—éœ€è¦è°ƒæ•´çš„åˆ†è´æ•°
            change_in_dBFS = self.target_db - original_db
            normalized_audio = audio.apply_gain(change_in_dBFS)
            
            # å¯¼å‡ºæ ‡å‡†åŒ–åçš„éŸ³é¢‘
            normalized_audio.export(output_path, format=format)
            
            print(f"âœ… éŸ³é¢‘éŸ³é‡æ ‡å‡†åŒ–å®Œæˆ: {original_db:.1f}dB -> {self.target_db:.1f}dB")
            return output_path
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æ ‡å‡†åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def get_audio_duration(self, audio_file: str) -> float:
        """
        è·å–éŸ³é¢‘æ–‡ä»¶æ—¶é•¿
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        try:
            cmd = ['ffmpeg', '-i', audio_file]
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE
            )
            _, stderr = process.communicate()
            output = stderr.decode('utf-8', errors='ignore')
            
            # è§£ææ—¶é•¿ä¿¡æ¯
            duration_lines = [line for line in output.split('\n') if 'Duration' in line]
            if not duration_lines:
                raise ValueError("æ— æ³•ä»FFmpegè¾“å‡ºä¸­æ‰¾åˆ°æ—¶é•¿ä¿¡æ¯")
            
            duration_str = duration_lines[0].split('Duration: ')[1].split(',')[0]
            duration_parts = duration_str.split(':')
            
            duration = (
                float(duration_parts[0]) * 3600 + 
                float(duration_parts[1]) * 60 + 
                float(duration_parts[2])
            )
            
            return duration
            
        except Exception as e:
            print(f"âš ï¸  è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {str(e)}")
            return 0.0
    
    def split_audio_by_silence(self, audio_file: str) -> List[Tuple[float, float]]:
        """
        åŸºäºé™é»˜æ£€æµ‹æ™ºèƒ½åˆ†å‰²éŸ³é¢‘
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†æ®µåˆ—è¡¨, æ¯ä¸ªå…ƒç´ ä¸º(å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´)çš„å…ƒç»„
            
        Note: ä¿®å¤åŸä»£ç ç¼ºé™· - æ”¹è¿›äº†é™é»˜æ£€æµ‹ç®—æ³•å’Œè¾¹ç•Œå¤„ç†
        """
        print(f"ğŸ™ï¸ å¼€å§‹éŸ³é¢‘æ™ºèƒ½åˆ†æ®µ: {audio_file}")
        
        if not os.path.exists(audio_file):
            raise FileNotFoundError(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        
        try:
            audio = AudioSegment.from_file(audio_file)
            duration = float(mediainfo(audio_file)["duration"])
            
            print(f"ğŸ“ éŸ³é¢‘æ€»æ—¶é•¿: {duration:.1f}ç§’")
            
            # å¦‚æœéŸ³é¢‘é•¿åº¦åœ¨å…è®¸èŒƒå›´å†…ï¼Œä¸éœ€è¦åˆ†å‰²
            if duration <= self.target_segment_length + self.silence_window:
                print("ğŸ“ éŸ³é¢‘é•¿åº¦é€‚ä¸­ï¼Œæ— éœ€åˆ†å‰²")
                return [(0, duration)]
            
            segments = []
            pos = 0.0
            safe_margin = 0.5  # é™é»˜ç‚¹å‰åå®‰å…¨è¾¹ç•Œ
            
            while pos < duration:
                # å¦‚æœå‰©ä½™æ—¶é•¿ä¸è¶…è¿‡ç›®æ ‡é•¿åº¦ï¼Œç›´æ¥ä½œä¸ºæœ€åä¸€æ®µ
                if duration - pos <= self.target_segment_length:
                    segments.append((pos, duration))
                    break
                
                # è®¡ç®—åˆ†å‰²ç‚¹æœç´¢åŒºé—´
                threshold = pos + self.target_segment_length
                window_start = int((threshold - self.silence_window) * 1000)
                window_end = int((threshold + self.silence_window) * 1000)
                
                # ç¡®ä¿çª—å£ä¸è¶…å‡ºéŸ³é¢‘èŒƒå›´
                window_end = min(window_end, len(audio))
                
                if window_start >= window_end:
                    # çª—å£æ— æ•ˆï¼Œä½¿ç”¨é˜ˆå€¼ç‚¹åˆ†å‰²
                    segments.append((pos, threshold))
                    pos = threshold
                    continue
                
                # åœ¨çª—å£å†…æ£€æµ‹é™é»˜åŒºåŸŸ
                silence_regions = detect_silence(
                    audio[window_start:window_end],
                    min_silence_len=int(safe_margin * 1000),
                    silence_thresh=-30
                )
                
                # å°†é™é»˜åŒºåŸŸæ—¶é—´è½¬æ¢ä¸ºç»å¯¹æ—¶é—´
                silence_regions = [
                    (s/1000 + (threshold - self.silence_window), 
                     e/1000 + (threshold - self.silence_window))
                    for s, e in silence_regions
                ]
                
                # ç­›é€‰æœ‰æ•ˆçš„é™é»˜åŒºåŸŸ
                valid_regions = [
                    (start, end) for start, end in silence_regions 
                    if (end - start) >= (safe_margin * 2) and 
                       threshold <= start + safe_margin <= threshold + self.silence_window
                ]
                
                if valid_regions:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆé™é»˜åŒºåŸŸ
                    start, end = valid_regions[0]
                    split_at = start + safe_margin
                    print(f"ğŸ¯ åœ¨é™é»˜åŒºåŸŸåˆ†å‰²: {split_at:.1f}ç§’")
                else:
                    # æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„é™é»˜åŒºåŸŸï¼Œä½¿ç”¨é˜ˆå€¼ç‚¹
                    split_at = threshold
                    print(f"âš ï¸  æœªæ‰¾åˆ°é™é»˜åŒºåŸŸï¼Œåœ¨é˜ˆå€¼ç‚¹åˆ†å‰²: {split_at:.1f}ç§’")
                
                segments.append((pos, split_at))
                pos = split_at
            
            print(f"âœ… éŸ³é¢‘åˆ†å‰²å®Œæˆï¼Œå…±{len(segments)}ä¸ªç‰‡æ®µ")
            
            # æ‰“å°åˆ†æ®µä¿¡æ¯
            for i, (start, end) in enumerate(segments):
                print(f"  ğŸ“ ç‰‡æ®µ{i+1}: {start:.1f}s - {end:.1f}s ({end-start:.1f}s)")
            
            return segments
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘åˆ†å‰²å¤±è´¥: {str(e)}")
            raise
    
    def process_transcription_result(self, result: Dict) -> pd.DataFrame:
        """
        å¤„ç†è½¬å½•ç»“æœ, è½¬æ¢ä¸ºæ ‡å‡†DataFrameæ ¼å¼
        
        Args:
            result: ASRå¼•æ“è¿”å›çš„è½¬å½•ç»“æœ
            
        Returns:
            å¤„ç†åçš„DataFrame
            
        Note: ä¿®å¤åŸä»£ç ç¼ºé™· - å¢åŠ äº†æ•°æ®éªŒè¯å’Œå¼‚å¸¸å¤„ç†
        """
        print("ğŸ“Š æ­£åœ¨å¤„ç†è½¬å½•ç»“æœ...")
        
        if 'segments' not in result:
            raise ValueError("âŒ è½¬å½•ç»“æœæ ¼å¼é”™è¯¯ï¼šç¼ºå°‘segmentså­—æ®µ")
        
        all_words = []
        
        for segment_idx, segment in enumerate(result['segments']):
            speaker_id = segment.get('speaker_id', None)
            
            if 'words' not in segment:
                print(f"âš ï¸  æ®µè½{segment_idx}ç¼ºå°‘wordså­—æ®µï¼Œè·³è¿‡")
                continue
            
            for word_idx, word in enumerate(segment['words']):
                try:
                    # æ£€æŸ¥è¯é•¿åº¦
                    word_text = word.get("word", "").strip()
                    if len(word_text) > 30:
                        print(f"âš ï¸  æ£€æµ‹åˆ°è¿‡é•¿è¯æ±‡ï¼Œè·³è¿‡: {word_text[:30]}...")
                        continue
                    
                    if not word_text:
                        print(f"âš ï¸  æ£€æµ‹åˆ°ç©ºè¯æ±‡ï¼Œè·³è¿‡")
                        continue
                    
                    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼ˆæ³•è¯­å¼•å·ç­‰ï¼‰
                    word_text = word_text.replace('Â»', '').replace('Â«', '')
                    
                    # å¤„ç†æ—¶é—´æˆ³
                    if 'start' not in word or 'end' not in word:
                        # ä½¿ç”¨å‰ä¸€ä¸ªè¯çš„ç»“æŸæ—¶é—´æˆ–å¯»æ‰¾ä¸‹ä¸€ä¸ªæœ‰æ—¶é—´æˆ³çš„è¯
                        if all_words:
                            start_time = end_time = all_words[-1]['end']
                        else:
                            # å¯»æ‰¾ä¸‹ä¸€ä¸ªæœ‰æ—¶é—´æˆ³çš„è¯
                            next_word = next(
                                (w for w in segment['words'][word_idx+1:] 
                                 if 'start' in w and 'end' in w), 
                                None
                            )
                            if next_word:
                                start_time = end_time = next_word["start"]
                            else:
                                print(f"âš ï¸  æ— æ³•ç¡®å®šè¯æ±‡æ—¶é—´æˆ³: {word_text}")
                                continue
                    else:
                        start_time = word.get('start', 0)
                        end_time = word['end']
                    
                    word_dict = {
                        'text': word_text,
                        'start': start_time,
                        'end': end_time,
                        'speaker_id': speaker_id
                    }
                    
                    all_words.append(word_dict)
                    
                except Exception as e:
                    print(f"âš ï¸  å¤„ç†è¯æ±‡æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        if not all_words:
            raise ValueError("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„è½¬å½•ç»“æœ")
        
        df = pd.DataFrame(all_words)
        print(f"âœ… è½¬å½•ç»“æœå¤„ç†å®Œæˆï¼Œå…±{len(df)}ä¸ªè¯æ±‡")
        
        return df
    
    def save_transcription_results(self, df: pd.DataFrame) -> str:
        """
        ä¿å­˜è½¬å½•ç»“æœåˆ°Excelæ–‡ä»¶
        
        Args:
            df: è½¬å½•ç»“æœDataFrame
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            
        Note: ä¿®å¤åŸä»£ç ç¼ºé™· - å¢åŠ äº†æ•°æ®æ¸…ç†å’ŒéªŒè¯
        """
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜è½¬å½•ç»“æœ...")
        
        # æ•°æ®æ¸…ç†
        initial_rows = len(df)
        
        # ç§»é™¤ç©ºæ–‡æœ¬è¡Œ
        df = df[df['text'].str.len() > 0]
        removed_empty = initial_rows - len(df)
        if removed_empty > 0:
            print(f"ğŸ§¹ ç§»é™¤äº†{removed_empty}è¡Œç©ºæ–‡æœ¬")
        
        # æ£€æŸ¥å¹¶ç§»é™¤è¿‡é•¿è¯æ±‡
        long_words = df[df['text'].str.len() > 30]
        if not long_words.empty:
            print(f"âš ï¸  æ£€æµ‹åˆ°{len(long_words)}ä¸ªè¿‡é•¿è¯æ±‡ï¼Œå·²ç§»é™¤")
            df = df[df['text'].str.len() <= 30]
        
        # ä¸ºæ–‡æœ¬æ·»åŠ å¼•å·ï¼ˆExcelæ ¼å¼è¦æ±‚ï¼‰
        df['text'] = df['text'].apply(lambda x: f'"{x}"')
        
        # ä¿å­˜åˆ°Excel
        try:
            df.to_excel(self.cleaned_chunks_file, index=False)
            print(f"âœ… è½¬å½•ç»“æœå·²ä¿å­˜: {self.cleaned_chunks_file}")
            print(f"ğŸ“ˆ æœ€ç»ˆæ•°æ®ç»Ÿè®¡: {len(df)}è¡Œè®°å½•")
            
            return str(self.cleaned_chunks_file)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜è½¬å½•ç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def transcribe_audio(self, 
                        audio_file: str,
                        vocal_audio_file: Optional[str] = None,
                        start_time: float = 0,
                        end_time: Optional[float] = None,
                        transcribe_func: Optional[Callable] = None) -> Dict:
        """
        è½¬å½•éŸ³é¢‘ç‰‡æ®µ
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            vocal_audio_file: äººå£°åˆ†ç¦»åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            transcribe_func: è½¬å½•å‡½æ•°ï¼ˆç”±å¤–éƒ¨ASRå¼•æ“æä¾›ï¼‰
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        if transcribe_func is None:
            raise ValueError("âŒ æœªæä¾›è½¬å½•å‡½æ•°ï¼Œè¯·æŒ‡å®šASRå¼•æ“")
        
        print(f"ğŸ¤ è½¬å½•éŸ³é¢‘ç‰‡æ®µ: {start_time:.1f}s - {end_time or 'ç»“æŸ'}s")
        
        try:
            result = transcribe_func(audio_file, vocal_audio_file, start_time, end_time)
            return result
        except Exception as e:
            print(f"âŒ éŸ³é¢‘è½¬å½•å¤±è´¥: {str(e)}")
            raise
    
    def transcribe_video_complete(self, 
                                video_file: str,
                                use_vocal_separation: bool = False,
                                transcribe_func: Optional[Callable] = None) -> str:
        """
        å®Œæ•´çš„è§†é¢‘è½¬å½•æµç¨‹
        
        Args:
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            use_vocal_separation: æ˜¯å¦ä½¿ç”¨äººå£°åˆ†ç¦»
            transcribe_func: è½¬å½•å‡½æ•°
            
        Returns:
            ä¿å­˜çš„è½¬å½•ç»“æœæ–‡ä»¶è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´è§†é¢‘è½¬å½•æµç¨‹...")
        
        try:
            # 1. è§†é¢‘è½¬éŸ³é¢‘
            audio_file = self.convert_video_to_audio(video_file)
            
            # 2. äººå£°åˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
            if use_vocal_separation:
                print("ğŸµ æ­£åœ¨è¿›è¡Œäººå£°åˆ†ç¦»...")
                # è¿™é‡Œéœ€è¦å¤–éƒ¨æä¾›äººå£°åˆ†ç¦»å‡½æ•°
                vocal_audio = str(self.vocal_audio_file)
                # vocal_separation_func(audio_file, vocal_audio)  # éœ€è¦å¤–éƒ¨å®ç°
            else:
                vocal_audio = audio_file
            
            # 3. éŸ³é¢‘åˆ†æ®µ
            segments = self.split_audio_by_silence(audio_file)
            
            # 4. åˆ†æ®µè½¬å½•
            all_results = []
            for i, (start, end) in enumerate(segments):
                print(f"ğŸ¤ è½¬å½•ç¬¬{i+1}/{len(segments)}ä¸ªç‰‡æ®µ...")
                result = self.transcribe_audio(
                    audio_file, vocal_audio, start, end, transcribe_func
                )
                all_results.append(result)
            
            # 5. åˆå¹¶ç»“æœ
            combined_result = {'segments': []}
            for result in all_results:
                if 'segments' in result:
                    combined_result['segments'].extend(result['segments'])
            
            # 6. å¤„ç†è½¬å½•ç»“æœ
            df = self.process_transcription_result(combined_result)
            
            # 7. ä¿å­˜ç»“æœ
            output_file = self.save_transcription_results(df)
            
            print("ğŸ‰ è§†é¢‘è½¬å½•æµç¨‹å®Œæˆï¼")
            return output_file
            
        except Exception as e:
            print(f"ğŸ’¥ è½¬å½•æµç¨‹å¤±è´¥: {str(e)}")
            raise


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºè½¬å½•å™¨å®ä¾‹
    transcriber = AudioTranscriber()
    
    # æµ‹è¯•è§†é¢‘æ–‡ä»¶è¾“å…¥
    if len(sys.argv) > 1:
        video_file = sys.argv[1]
    else:
        video_file = input('è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„: ')
    
    if not video_file.strip():
        print("âŒ è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
        sys.exit(1)
    
    try:
        # æµ‹è¯•è§†é¢‘è½¬éŸ³é¢‘
        print("\nğŸ§ª æµ‹è¯•è§†é¢‘è½¬éŸ³é¢‘...")
        audio_file = transcriber.convert_video_to_audio(video_file)
        
        # æµ‹è¯•è·å–éŸ³é¢‘æ—¶é•¿
        print("\nğŸ“ æµ‹è¯•è·å–éŸ³é¢‘æ—¶é•¿...")
        duration = transcriber.get_audio_duration(audio_file)
        print(f"â±ï¸  éŸ³é¢‘æ—¶é•¿: {duration:.1f}ç§’")
        
        # æµ‹è¯•éŸ³é¢‘åˆ†æ®µ
        print("\nâœ‚ï¸  æµ‹è¯•éŸ³é¢‘åˆ†æ®µ...")
        segments = transcriber.split_audio_by_silence(audio_file)
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_file}")
        print(f"ğŸ“Š åˆ†æ®µæ•°é‡: {len(segments)}")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1) 