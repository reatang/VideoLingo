"""
# ----------------------------------------------------------------------------
# å†…å®¹æ€»ç»“å™¨æ¨¡å— - æ•´ä¸ªç¿»è¯‘æµç¨‹çš„æ ¸å¿ƒæ¡¥æ¢
# 
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. è§†é¢‘å†…å®¹ä¸»é¢˜æ€»ç»“å’Œç†è§£
# 2. ä¸“ä¸šæœ¯è¯­æ™ºèƒ½è¯†åˆ«å’Œæå–
# 3. ç¿»è¯‘ä¸Šä¸‹æ–‡ä¿¡æ¯æ„å»º
# 4. æœ¯è¯­ä¸€è‡´æ€§ç®¡ç†ç³»ç»Ÿ
# 5. è‡ªå®šä¹‰æœ¯è¯­åº“é›†æˆç®¡ç†
# 
# è¾“å…¥ï¼šåˆ†å‰²åçš„æ–‡æœ¬æ–‡ä»¶ï¼Œè‡ªå®šä¹‰æœ¯è¯­è¡¨
# è¾“å‡ºï¼šä¸»é¢˜æ€»ç»“JSONï¼Œæœ¯è¯­å­—å…¸JSONï¼Œç¿»è¯‘ä¸Šä¸‹æ–‡ä¿¡æ¯
# 
# è®¾è®¡åŸåˆ™ï¼š
# - ç¡®ä¿ç¿»è¯‘è´¨é‡çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
# - æ”¯æŒå¤šè¯­è¨€å’Œé¢†åŸŸè‡ªé€‚åº”
# - æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ”¯æ’‘åç»­ç¿»è¯‘
# - æ™ºèƒ½æœ¯è¯­ç®¡ç†å’Œå†²çªè§£å†³
# ----------------------------------------------------------------------------
"""

import os
import json
import hashlib
from typing import List, Dict, Optional, Callable, Set, Tuple
from pathlib import Path
import concurrent.futures
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import re


@dataclass
class Term:
    """æœ¯è¯­æ•°æ®ç±»"""
    src: str                 # æºè¯­è¨€æœ¯è¯­
    tgt: str                 # ç›®æ ‡è¯­è¨€ç¿»è¯‘
    note: str                # æœ¯è¯­è¯´æ˜
    confidence: float = 1.0  # ç½®ä¿¡åº¦
    category: str = ""       # æœ¯è¯­ç±»åˆ«
    frequency: int = 1       # å‡ºç°é¢‘ç‡
    context: List[str] = None # ä¸Šä¸‹æ–‡ç¤ºä¾‹
    
    def __post_init__(self):
        if self.context is None:
            self.context = []


@dataclass  
class ContentSummary:
    """å†…å®¹æ€»ç»“æ•°æ®ç±»"""
    theme: str                    # ä¸»é¢˜æ€»ç»“
    domain: str = ""             # é¢†åŸŸåˆ†ç±»
    style: str = ""              # å†…å®¹é£æ ¼
    target_audience: str = ""    # ç›®æ ‡å—ä¼—
    key_concepts: List[str] = None # å…³é”®æ¦‚å¿µ
    complexity_level: str = ""   # å†…å®¹å¤æ‚åº¦
    
    def __post_init__(self):
        if self.key_concepts is None:
            self.key_concepts = []


class ContentSummarizer:
    """å†…å®¹æ€»ç»“å™¨ç±» - ç¿»è¯‘æµç¨‹çš„æ ¸å¿ƒæ¡¥æ¢"""
    
    def __init__(self,
                 input_file: str = 'output/log/3_2_split_by_meaning.txt',
                 custom_terms_file: str = 'custom_terms.xlsx',
                 output_dir: str = 'output/log',
                 src_language: str = 'en',
                 tgt_language: str = 'zh',
                 summary_length: int = 5000,
                 max_terms: int = 30,
                 min_term_frequency: int = 2,
                 max_workers: int = 4):
        """
        åˆå§‹åŒ–å†…å®¹æ€»ç»“å™¨
        
        Args:
            input_file: è¾“å…¥çš„åˆ†å‰²æ–‡æœ¬æ–‡ä»¶
            custom_terms_file: è‡ªå®šä¹‰æœ¯è¯­è¡¨æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•
            src_language: æºè¯­è¨€ä»£ç 
            tgt_language: ç›®æ ‡è¯­è¨€ä»£ç 
            summary_length: ç”¨äºæ€»ç»“çš„æ–‡æœ¬é•¿åº¦é™åˆ¶
            max_terms: æœ€å¤§æœ¯è¯­æå–æ•°é‡
            min_term_frequency: æœ¯è¯­æœ€å°é¢‘ç‡é˜ˆå€¼
            max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°
        """
        self.input_file = Path(input_file)
        self.custom_terms_file = Path(custom_terms_file)
        self.output_dir = Path(output_dir)
        self.src_language = src_language
        self.tgt_language = tgt_language
        self.summary_length = summary_length
        self.max_terms = max_terms
        self.min_term_frequency = min_term_frequency
        self.max_workers = max_workers
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.summary_file = self.output_dir / '4_1_content_summary.json'
        self.terminology_file = self.output_dir / '4_1_terminology.json'
        self.context_file = self.output_dir / '4_1_translation_context.json'
        self.cache_file = self.output_dir / '4_1_summary_cache.json'
        
        # é¢†åŸŸå…³é”®è¯é…ç½®ï¼ˆç”¨äºè‡ªåŠ¨é¢†åŸŸè¯†åˆ«ï¼‰
        self.domain_keywords = {
            'technology': ['technology', 'software', 'algorithm', 'AI', 'machine learning', 'data', 'programming', 'system', 'network'],
            'medicine': ['medical', 'health', 'disease', 'treatment', 'patient', 'diagnosis', 'medicine', 'therapy', 'clinical'],
            'business': ['business', 'market', 'financial', 'economy', 'investment', 'company', 'strategy', 'management', 'revenue'],
            'education': ['education', 'learning', 'student', 'teacher', 'school', 'university', 'course', 'academic', 'study'],
            'science': ['science', 'research', 'experiment', 'theory', 'analysis', 'study', 'data', 'method', 'result'],
            'entertainment': ['movie', 'music', 'game', 'entertainment', 'show', 'performance', 'art', 'culture', 'story']
        }
        
        # æ‡’åŠ è½½ä¾èµ–
        self._pd = None
        self._ask_gpt_func = None
        
        # å†…éƒ¨çŠ¶æ€
        self._content_cache = {}
        self._custom_terms = []
        self._extracted_terms = []
        
    def _get_pandas(self):
        """æ‡’åŠ è½½pandas"""
        if self._pd is None:
            try:
                import pandas as pd
                self._pd = pd
            except ImportError:
                raise ImportError("âŒ æœªæ‰¾åˆ°pandasåº“, è¯·å®‰è£…: pip install pandas")
        return self._pd
    
    def set_gpt_function(self, ask_gpt_func: Callable):
        """
        è®¾ç½®GPTè°ƒç”¨å‡½æ•°
        
        Args:
            ask_gpt_func: GPT APIè°ƒç”¨å‡½æ•°ï¼Œç­¾åä¸º(prompt, resp_type='json', log_title='', valid_def=None)
        """
        self._ask_gpt_func = ask_gpt_func
        print("âœ… GPTå‡½æ•°å·²è®¾ç½®")
    
    def _calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œï¼Œç”¨äºç¼“å­˜"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def load_input_content(self) -> str:
        """
        åŠ è½½å¹¶é¢„å¤„ç†è¾“å…¥å†…å®¹
        
        Returns:
            å¤„ç†åçš„æ–‡æœ¬å†…å®¹
        """
        print(f"ğŸ“– æ­£åœ¨åŠ è½½åˆ†å‰²æ–‡æœ¬: {self.input_file}")
        
        if not self.input_file.exists():
            raise FileNotFoundError(f"âŒ åˆ†å‰²æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {self.input_file}")
        
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ¸…ç†å’Œåˆå¹¶æ–‡æœ¬
            cleaned_lines = [line.strip() for line in lines if line.strip()]
            combined_text = ' '.join(cleaned_lines)
            
            # é™åˆ¶æ€»ç»“é•¿åº¦ï¼Œé¿å…è¶…å‡ºGPTé™åˆ¶
            if len(combined_text) > self.summary_length:
                print(f"ğŸ“ æ–‡æœ¬é•¿åº¦è¶…é™ï¼Œæˆªå–å‰{self.summary_length}ä¸ªå­—ç¬¦")
                combined_text = combined_text[:self.summary_length]
            
            print(f"âœ… åŠ è½½äº†{len(cleaned_lines)}è¡Œæ–‡æœ¬ï¼Œå…±{len(combined_text)}ä¸ªå­—ç¬¦")
            return combined_text
            
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†å‰²æ–‡æœ¬å¤±è´¥: {str(e)}")
            raise
    
    def load_custom_terms(self) -> List[Term]:
        """
        åŠ è½½è‡ªå®šä¹‰æœ¯è¯­è¡¨
        
        Returns:
            è‡ªå®šä¹‰æœ¯è¯­åˆ—è¡¨
        """
        print(f"ğŸ“š æ­£åœ¨åŠ è½½è‡ªå®šä¹‰æœ¯è¯­è¡¨...")
        
        custom_terms = []
        
        if not self.custom_terms_file.exists():
            print(f"âš ï¸  è‡ªå®šä¹‰æœ¯è¯­è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {self.custom_terms_file}")
            return custom_terms
        
        try:
            pd = self._get_pandas()
            df = pd.read_excel(self.custom_terms_file)
            
            # å‡è®¾Excelæ ¼å¼ä¸º: æºæœ¯è¯­ | ç›®æ ‡ç¿»è¯‘ | è¯´æ˜
            for _, row in df.iterrows():
                if len(row) >= 3 and pd.notna(row.iloc[0]):
                    term = Term(
                        src=str(row.iloc[0]).strip(),
                        tgt=str(row.iloc[1]).strip() if pd.notna(row.iloc[1]) else "",
                        note=str(row.iloc[2]).strip() if pd.notna(row.iloc[2]) else "",
                        confidence=1.0,  # è‡ªå®šä¹‰æœ¯è¯­ç½®ä¿¡åº¦æœ€é«˜
                        category="custom"
                    )
                    custom_terms.append(term)
            
            print(f"âœ… åŠ è½½äº†{len(custom_terms)}ä¸ªè‡ªå®šä¹‰æœ¯è¯­")
            return custom_terms
            
        except Exception as e:
            print(f"âŒ åŠ è½½è‡ªå®šä¹‰æœ¯è¯­è¡¨å¤±è´¥: {str(e)}")
            return custom_terms
    
    def _detect_domain_and_style(self, content: str) -> Tuple[str, str, str]:
        """
        æ£€æµ‹å†…å®¹é¢†åŸŸã€é£æ ¼å’Œç›®æ ‡å—ä¼—
        
        Args:
            content: æ–‡æœ¬å†…å®¹
            
        Returns:
            (é¢†åŸŸ, é£æ ¼, ç›®æ ‡å—ä¼—)
        """
        content_lower = content.lower()
        
        # é¢†åŸŸæ£€æµ‹
        domain_scores = {}
        for domain, keywords in self.domain_keywords.items():
            score = sum(content_lower.count(keyword) for keyword in keywords)
            domain_scores[domain] = score
        
        detected_domain = max(domain_scores, key=domain_scores.get) if max(domain_scores.values()) > 0 else "general"
        
        # é£æ ¼æ£€æµ‹ï¼ˆåŸºäºç®€å•å¯å‘å¼ï¼‰
        if any(word in content_lower for word in ['tutorial', 'how to', 'step', 'guide']):
            style = "tutorial"
        elif any(word in content_lower for word in ['analysis', 'research', 'study', 'data']):
            style = "analytical"
        elif any(word in content_lower for word in ['news', 'report', 'today', 'recent']):
            style = "news"
        else:
            style = "general"
        
        # ç›®æ ‡å—ä¼—æ£€æµ‹
        if detected_domain in ['technology', 'science']:
            audience = "technical"
        elif detected_domain in ['education']:
            audience = "educational"
        elif detected_domain in ['business']:
            audience = "professional"
        else:
            audience = "general"
        
        return detected_domain, style, audience
    
    def generate_content_summary(self, content: str, custom_terms: List[Term]) -> ContentSummary:
        """
        ç”Ÿæˆå†…å®¹æ€»ç»“
        
        Args:
            content: æ–‡æœ¬å†…å®¹
            custom_terms: è‡ªå®šä¹‰æœ¯è¯­åˆ—è¡¨
            
        Returns:
            å†…å®¹æ€»ç»“å¯¹è±¡
        """
        print("ğŸ§  æ­£åœ¨ç”Ÿæˆå†…å®¹æ€»ç»“...")
        
        if self._ask_gpt_func is None:
            print("âš ï¸  æœªè®¾ç½®GPTå‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤æ€»ç»“")
            domain, style, audience = self._detect_domain_and_style(content)
            return ContentSummary(
                theme=f"è¿™æ˜¯ä¸€ä¸ªå…³äº{domain}é¢†åŸŸçš„{style}å†…å®¹ï¼Œä¸»è¦é¢å‘{audience}å—ä¼—ã€‚",
                domain=domain,
                style=style,
                target_audience=audience,
                key_concepts=[],
                complexity_level="medium"
            )
        
        try:
            # æ„å»ºæ€»ç»“æç¤ºè¯
            summary_prompt = self._get_summary_prompt(content, custom_terms)
            
            # è°ƒç”¨GPTç”Ÿæˆæ€»ç»“
            response = self._ask_gpt_func(
                summary_prompt,
                resp_type='json',
                log_title='content_summary',
                valid_def=self._validate_summary_response
            )
            
            # è§£æå“åº”
            theme = response.get('theme', '')
            domain, style, audience = self._detect_domain_and_style(content)
            
            summary = ContentSummary(
                theme=theme,
                domain=response.get('domain', domain),
                style=response.get('style', style),
                target_audience=response.get('target_audience', audience),
                key_concepts=response.get('key_concepts', []),
                complexity_level=response.get('complexity_level', 'medium')
            )
            
            print(f"âœ… å†…å®¹æ€»ç»“ç”Ÿæˆå®Œæˆ")
            return summary
            
        except Exception as e:
            print(f"âŒ å†…å®¹æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}")
            # å›é€€åˆ°åŸºç¡€æ€»ç»“
            domain, style, audience = self._detect_domain_and_style(content)
            return ContentSummary(
                theme="è§†é¢‘å†…å®¹æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†æç»“æœã€‚",
                domain=domain,
                style=style,
                target_audience=audience
            )
    
    def _get_summary_prompt(self, content: str, custom_terms: List[Term]) -> str:
        """ç”Ÿæˆæ€»ç»“æç¤ºè¯"""
        
        # æ„å»ºè‡ªå®šä¹‰æœ¯è¯­æç¤º
        terms_note = ""
        if custom_terms:
            terms_list = [f"- {term.src}: {term.tgt} ({term.note})" for term in custom_terms]
            terms_note = f"\n### Existing Terms\nPlease consider these existing terms:\n" + "\n".join(terms_list)
        
        return f"""
## Role
You are a professional video content analyst and translation consultant, specializing in {self.src_language} comprehension and {self.tgt_language} localization strategy.

## Task
Analyze the provided {self.src_language} video content and generate a comprehensive summary for translation optimization:

1. **Content Theme**: Write 2-3 sentences summarizing the main topic and key points
2. **Domain Classification**: Identify the content domain (technology/medicine/business/education/science/entertainment/general)
3. **Content Style**: Determine the presentation style (tutorial/analytical/news/documentary/entertainment/general)
4. **Target Audience**: Identify the intended audience (technical/educational/professional/general)
5. **Key Concepts**: Extract 5-8 most important concepts or themes
6. **Complexity Level**: Assess content complexity (beginner/intermediate/advanced)

{terms_note}

## Guidelines
- Focus on information that will help translators understand context and maintain consistency
- Consider cultural adaptation needs for {self.tgt_language} audience
- Identify any domain-specific language patterns or terminology requirements

## INPUT
<content>
{content}
</content>

## Output in only JSON format and no other text
```json
{{
    "theme": "2-3 sentence comprehensive summary of the video content",
    "domain": "technology|medicine|business|education|science|entertainment|general",
    "style": "tutorial|analytical|news|documentary|entertainment|general",
    "target_audience": "technical|educational|professional|general",
    "key_concepts": ["concept1", "concept2", "concept3"],
    "complexity_level": "beginner|intermediate|advanced"
}}
```

Note: Start your answer with ```json and end with ```, do not add any other text.
""".strip()
    
    def extract_terminology(self, content: str, custom_terms: List[Term]) -> List[Term]:
        """
        æå–ä¸“ä¸šæœ¯è¯­
        
        Args:
            content: æ–‡æœ¬å†…å®¹
            custom_terms: è‡ªå®šä¹‰æœ¯è¯­åˆ—è¡¨
            
        Returns:
            æå–çš„æœ¯è¯­åˆ—è¡¨
        """
        print("ğŸ” æ­£åœ¨æå–ä¸“ä¸šæœ¯è¯­...")
        
        if self._ask_gpt_func is None:
            print("âš ï¸  æœªè®¾ç½®GPTå‡½æ•°ï¼Œä½¿ç”¨åŸºç¡€æœ¯è¯­æå–")
            return self._extract_terms_fallback(content, custom_terms)
        
        try:
            # æ„å»ºæœ¯è¯­æå–æç¤ºè¯
            terminology_prompt = self._get_terminology_prompt(content, custom_terms)
            
            # è°ƒç”¨GPTæå–æœ¯è¯­
            response = self._ask_gpt_func(
                terminology_prompt,
                resp_type='json',
                log_title='terminology_extraction',
                valid_def=self._validate_terminology_response
            )
            
            # è§£ææœ¯è¯­
            extracted_terms = []
            for term_data in response.get('terms', []):
                term = Term(
                    src=term_data.get('src', ''),
                    tgt=term_data.get('tgt', ''),
                    note=term_data.get('note', ''),
                    confidence=term_data.get('confidence', 0.8),
                    category=term_data.get('category', 'extracted'),
                    frequency=self._count_term_frequency(term_data.get('src', ''), content)
                )
                extracted_terms.append(term)
            
            # åˆå¹¶è‡ªå®šä¹‰æœ¯è¯­å’Œæå–æœ¯è¯­
            all_terms = custom_terms + extracted_terms
            
            # å»é‡å’Œä¼˜åŒ–
            unique_terms = self._deduplicate_terms(all_terms)
            
            print(f"âœ… æœ¯è¯­æå–å®Œæˆï¼Œå…±{len(unique_terms)}ä¸ªæœ¯è¯­")
            return unique_terms
            
        except Exception as e:
            print(f"âŒ æœ¯è¯­æå–å¤±è´¥: {str(e)}")
            return self._extract_terms_fallback(content, custom_terms)
    
    def _get_terminology_prompt(self, content: str, custom_terms: List[Term]) -> str:
        """ç”Ÿæˆæœ¯è¯­æå–æç¤ºè¯"""
        
        # æ„å»ºæ’é™¤æœ¯è¯­åˆ—è¡¨
        exclude_terms = [term.src for term in custom_terms]
        exclude_note = ""
        if exclude_terms:
            exclude_note = f"\n### Exclude These Terms\nDo not extract these terms as they are already defined:\n{', '.join(exclude_terms)}"
        
        return f"""
## Role
You are a professional terminology expert specializing in {self.src_language} to {self.tgt_language} translation, with deep understanding of domain-specific terminology.

## Task
Extract and translate professional terms from the provided {self.src_language} content:

1. **Identify Terms**: Find technical terms, proper nouns, concepts, and domain-specific vocabulary
2. **Provide Translations**: Give accurate {self.tgt_language} translations or keep original if appropriate
3. **Add Explanations**: Brief explanations to help translators understand context and usage
4. **Categorize Terms**: Classify terms by type (technical/proper_noun/concept/acronym/general)
5. **Assess Confidence**: Rate translation confidence (0.6-1.0)

{exclude_note}

## Extraction Guidelines
- Focus on terms that appear multiple times or are central to understanding
- Include acronyms, technical jargon, and specialized vocabulary
- Prioritize terms that might be difficult for general translators
- Extract no more than {self.max_terms} terms
- Ensure translations are contextually appropriate

## INPUT
<content>
{content}
</content>

## Output in only JSON format and no other text
```json
{{
    "terms": [
        {{
            "src": "{self.src_language} term or phrase",
            "tgt": "{self.tgt_language} translation or original if appropriate",
            "note": "Brief explanation of meaning and usage context",
            "category": "technical|proper_noun|concept|acronym|general",
            "confidence": 0.8
        }}
    ]
}}
```

Note: Start your answer with ```json and end with ```, do not add any other text.
""".strip()
    
    def _extract_terms_fallback(self, content: str, custom_terms: List[Term]) -> List[Term]:
        """æœ¯è¯­æå–çš„å¤‡ç”¨æ–¹æ¡ˆ"""
        print("ğŸ“ ä½¿ç”¨åŸºç¡€æœ¯è¯­æå–...")
        
        # ç®€å•çš„å¯å‘å¼æœ¯è¯­æå–
        terms = custom_terms.copy()
        
        # æå–å¤§å†™å•è¯å’Œç¼©å†™
        uppercase_pattern = r'\b[A-Z]{2,}\b'
        acronyms = re.findall(uppercase_pattern, content)
        
        # æå–å¸¸è§æœ¯è¯­æ¨¡å¼
        term_patterns = [
            r'\b[A-Z][a-z]+ [A-Z][a-z]+\b',  # ä¸“æœ‰åè¯
            r'\b\w+ing\b',                    # æŠ€æœ¯åŠ¨ä½œ
            r'\b\w+tion\b',                   # æ¦‚å¿µåè¯
        ]
        
        all_candidates = set(acronyms)
        for pattern in term_patterns:
            candidates = re.findall(pattern, content)
            all_candidates.update(candidates)
        
        # è½¬æ¢ä¸ºTermå¯¹è±¡
        for candidate in list(all_candidates)[:self.max_terms//2]:
            if len(candidate) > 2 and candidate.lower() not in [t.src.lower() for t in terms]:
                term = Term(
                    src=candidate,
                    tgt=candidate,  # ä¿æŒåŸæ–‡
                    note=f"Automatically extracted term",
                    confidence=0.6,
                    category="extracted",
                    frequency=content.lower().count(candidate.lower())
                )
                terms.append(term)
        
        return terms
    
    def _count_term_frequency(self, term: str, content: str) -> int:
        """è®¡ç®—æœ¯è¯­åœ¨å†…å®¹ä¸­çš„é¢‘ç‡"""
        return content.lower().count(term.lower())
    
    def _deduplicate_terms(self, terms: List[Term]) -> List[Term]:
        """å»é‡å’Œä¼˜åŒ–æœ¯è¯­åˆ—è¡¨"""
        # æŒ‰æºæœ¯è¯­å»é‡ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„
        unique_terms = {}
        for term in terms:
            key = term.src.lower()
            if key not in unique_terms or term.confidence > unique_terms[key].confidence:
                unique_terms[key] = term
        
        # æ’åºï¼šè‡ªå®šä¹‰æœ¯è¯­ä¼˜å…ˆï¼Œç„¶åæŒ‰é¢‘ç‡å’Œç½®ä¿¡åº¦
        sorted_terms = sorted(
            unique_terms.values(),
            key=lambda t: (t.category == "custom", t.frequency, t.confidence),
            reverse=True
        )
        
        return sorted_terms[:self.max_terms]
    
    def generate_translation_context(self, 
                                   summary: ContentSummary, 
                                   terms: List[Term]) -> Dict:
        """
        ç”Ÿæˆç¿»è¯‘ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            summary: å†…å®¹æ€»ç»“
            terms: æœ¯è¯­åˆ—è¡¨
            
        Returns:
            ç¿»è¯‘ä¸Šä¸‹æ–‡å­—å…¸
        """
        print("ğŸ”— æ­£åœ¨ç”Ÿæˆç¿»è¯‘ä¸Šä¸‹æ–‡...")
        
        # æ„å»ºæœ¯è¯­å­—å…¸ï¼ˆç”¨äºå¿«é€ŸæŸ¥è¯¢ï¼‰
        term_dict = {term.src.lower(): term for term in terms}
        
        # æŒ‰ç±»åˆ«åˆ†ç»„æœ¯è¯­
        terms_by_category = defaultdict(list)
        for term in terms:
            terms_by_category[term.category].append(term)
        
        # ç”Ÿæˆç¿»è¯‘æŒ‡å¯¼
        translation_guidelines = self._generate_translation_guidelines(summary, terms)
        
        context = {
            "content_summary": asdict(summary),
            "terminology": {
                "total_terms": len(terms),
                "by_category": {cat: len(terms) for cat, terms in terms_by_category.items()},
                "high_priority": [asdict(t) for t in terms if t.confidence > 0.8 or t.category == "custom"],
                "all_terms": [asdict(t) for t in terms]
            },
            "translation_guidelines": translation_guidelines,
            "quick_reference": {
                term.src: {"tgt": term.tgt, "note": term.note} 
                for term in terms if term.confidence > 0.7
            },
            "metadata": {
                "src_language": self.src_language,
                "tgt_language": self.tgt_language,
                "generation_time": self._get_timestamp(),
                "content_hash": self._calculate_content_hash(str(summary) + str(terms))
            }
        }
        
        print(f"âœ… ç¿»è¯‘ä¸Šä¸‹æ–‡ç”Ÿæˆå®Œæˆ")
        return context
    
    def _generate_translation_guidelines(self, summary: ContentSummary, terms: List[Term]) -> Dict:
        """ç”Ÿæˆç¿»è¯‘æŒ‡å¯¼åŸåˆ™"""
        guidelines = {
            "domain_specific": {
                "domain": summary.domain,
                "terminology_density": "high" if len(terms) > 20 else "medium" if len(terms) > 10 else "low",
                "technical_level": summary.complexity_level
            },
            "style_adaptation": {
                "source_style": summary.style,
                "target_audience": summary.target_audience,
                "formality_level": "formal" if summary.target_audience in ["technical", "professional"] else "casual"
            },
            "consistency_rules": {
                "maintain_terminology": [t.src for t in terms if t.category in ["custom", "technical"]],
                "cultural_adaptation": summary.domain in ["entertainment", "education"],
                "preserve_names": [t.src for t in terms if t.category == "proper_noun"]
            }
        }
        return guidelines
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def save_results(self, 
                    summary: ContentSummary, 
                    terms: List[Term], 
                    context: Dict) -> Tuple[str, str, str]:
        """
        ä¿å­˜æ‰€æœ‰ç»“æœ
        
        Args:
            summary: å†…å®¹æ€»ç»“
            terms: æœ¯è¯­åˆ—è¡¨
            context: ç¿»è¯‘ä¸Šä¸‹æ–‡
            
        Returns:
            (æ€»ç»“æ–‡ä»¶è·¯å¾„, æœ¯è¯­æ–‡ä»¶è·¯å¾„, ä¸Šä¸‹æ–‡æ–‡ä»¶è·¯å¾„)
        """
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
        
        try:
            # ä¿å­˜å†…å®¹æ€»ç»“
            with open(self.summary_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(summary), f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æœ¯è¯­ä¿¡æ¯ï¼ˆå…¼å®¹åŸæ ¼å¼ï¼‰
            terminology_data = {
                "theme": summary.theme,
                "terms": [asdict(term) for term in terms]
            }
            with open(self.terminology_file, 'w', encoding='utf-8') as f:
                json.dump(terminology_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ç¿»è¯‘ä¸Šä¸‹æ–‡
            with open(self.context_file, 'w', encoding='utf-8') as f:
                json.dump(context, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ç»“æœä¿å­˜å®Œæˆ")
            print(f"  ğŸ“„ å†…å®¹æ€»ç»“: {self.summary_file}")
            print(f"  ğŸ“š æœ¯è¯­ä¿¡æ¯: {self.terminology_file}")
            print(f"  ğŸ”— ç¿»è¯‘ä¸Šä¸‹æ–‡: {self.context_file}")
            
            return str(self.summary_file), str(self.terminology_file), str(self.context_file)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def _validate_summary_response(self, response_data: Dict) -> Dict:
        """éªŒè¯æ€»ç»“å“åº”æ ¼å¼"""
        if not isinstance(response_data, dict):
            return {"status": "error", "message": "Response must be a dictionary"}
        
        required_fields = ['theme']
        for field in required_fields:
            if field not in response_data:
                return {"status": "error", "message": f"Missing required field: {field}"}
        
        return {"status": "success", "message": "Summary validation passed"}
    
    def _validate_terminology_response(self, response_data: Dict) -> Dict:
        """éªŒè¯æœ¯è¯­å“åº”æ ¼å¼"""
        if not isinstance(response_data, dict):
            return {"status": "error", "message": "Response must be a dictionary"}
        
        if 'terms' not in response_data:
            return {"status": "error", "message": "Missing 'terms' field"}
        
        required_term_keys = {'src', 'tgt', 'note'}
        for term in response_data['terms']:
            if not all(key in term for key in required_term_keys):
                return {"status": "error", "message": "Invalid term format"}
        
        return {"status": "success", "message": "Terminology validation passed"}
    
    def process_complete_summarization(self) -> Tuple[str, str, str]:
        """
        å®Œæ•´çš„å†…å®¹æ€»ç»“å¤„ç†æµç¨‹
        
        Returns:
            (æ€»ç»“æ–‡ä»¶è·¯å¾„, æœ¯è¯­æ–‡ä»¶è·¯å¾„, ä¸Šä¸‹æ–‡æ–‡ä»¶è·¯å¾„)
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´å†…å®¹æ€»ç»“æµç¨‹...")
        
        try:
            # 1. åŠ è½½è¾“å…¥å†…å®¹å’Œè‡ªå®šä¹‰æœ¯è¯­
            content = self.load_input_content()
            custom_terms = self.load_custom_terms()
            
            # 2. ç”Ÿæˆå†…å®¹æ€»ç»“
            summary = self.generate_content_summary(content, custom_terms)
            
            # 3. æå–æœ¯è¯­
            terms = self.extract_terminology(content, custom_terms)
            
            # 4. ç”Ÿæˆç¿»è¯‘ä¸Šä¸‹æ–‡
            context = self.generate_translation_context(summary, terms)
            
            # 5. ä¿å­˜ç»“æœ
            file_paths = self.save_results(summary, terms, context)
            
            print("ğŸ‰ å†…å®¹æ€»ç»“æµç¨‹å®Œæˆï¼")
            print(f"ğŸ“Š æ€»ç»“ç»Ÿè®¡:")
            print(f"  ğŸ¯ ä¸»é¢˜: {summary.theme[:100]}...")
            print(f"  ğŸ·ï¸  é¢†åŸŸ: {summary.domain}")
            print(f"  ğŸ“ é£æ ¼: {summary.style}")
            print(f"  ğŸ‘¥ å—ä¼—: {summary.target_audience}")
            print(f"  ğŸ“š æœ¯è¯­æ•°é‡: {len(terms)}")
            print(f"  ğŸ”— ä¸Šä¸‹æ–‡ä¿¡æ¯: {len(context)} é¡¹")
            
            return file_paths
            
        except Exception as e:
            print(f"ğŸ’¥ å†…å®¹æ€»ç»“æµç¨‹å¤±è´¥: {str(e)}")
            raise


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºæ€»ç»“å™¨å®ä¾‹
    summarizer = ContentSummarizer(
        src_language='en',
        tgt_language='zh',
        summary_length=5000,
        max_terms=25
    )
    
    # æµ‹è¯•å‚æ•°
    test_with_gpt = '--gpt' in sys.argv
    
    if test_with_gpt:
        print("âš ï¸  æ³¨æ„: éœ€è¦æä¾›GPTå‡½æ•°æ‰èƒ½è¿›è¡Œå®Œæ•´æµ‹è¯•")
        # summarizer.set_gpt_function(your_gpt_function)
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not summarizer.input_file.exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {summarizer.input_file}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ–‡æœ¬åˆ†å‰²å™¨ç”Ÿæˆåˆ†å‰²æ–‡ä»¶")
            sys.exit(1)
        
        # è¿è¡Œå®Œæ•´æ€»ç»“æµç¨‹
        print("\nğŸ§ª æµ‹è¯•å†…å®¹æ€»ç»“æµç¨‹...")
        
        file_paths = summarizer.process_complete_summarization()
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        for i, path in enumerate(file_paths, 1):
            print(f"  {i}. {path}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        if Path(file_paths[1]).exists():  # æœ¯è¯­æ–‡ä»¶
            with open(file_paths[1], 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"\nğŸ“‹ æœ¯è¯­é¢„è§ˆ (å‰5ä¸ª):")
            for i, term in enumerate(data.get('terms', [])[:5], 1):
                print(f"  {i}. {term.get('src', '')} â†’ {term.get('tgt', '')} ({term.get('note', '')})")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1) 