"""
# ----------------------------------------------------------------------------
# è¯­ä¹‰æ–‡æœ¬åˆ†å‰²å™¨
# 
# åŸºäºGPTçš„æ™ºèƒ½è¯­ä¹‰åˆ†å‰²ï¼š
# 1. åˆ†æå¥å­ç»“æ„å’Œå¤æ‚åº¦
# 2. ç”Ÿæˆå¤šç§åˆ†å‰²æ–¹æ¡ˆå¹¶æ¯”è¾ƒ
# 3. é€‰æ‹©æœ€ä½³åˆ†å‰²ç‚¹
# 4. åŒæ­¥å¤„ç†ç¡®ä¿ç¨³å®šæ€§
# ----------------------------------------------------------------------------
"""

import math
from pathlib import Path
from typing import List, Optional
from difflib import SequenceMatcher

from modules.gpt import ask_gpt
from modules.config import get_config_manager
from modules.common_utils import paths


class SemanticSplitter:
    """åŸºäºè¯­ä¹‰çš„æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self, 
                 output_dir: str = "output",
                 max_split_length: int = 20,
                 retry_attempts: int = 3):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†å‰²å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            max_split_length: æœ€å¤§åˆ†å‰²é•¿åº¦
            retry_attempts: é‡è¯•æ¬¡æ•°
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.max_split_length = max_split_length
        self.retry_attempts = retry_attempts
        
        # è·å–é…ç½®ç®¡ç†å™¨
        try:
            self.config = get_config_manager()
            self.language = self._get_language()
            self.joiner = self._get_joiner()
        except Exception as e:
            print(f"âš ï¸  é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.language = "auto"
            self.joiner = " "
    
    def _get_language(self) -> str:
        """è·å–æ£€æµ‹åˆ°çš„è¯­è¨€"""
        try:
            whisper_language = self.config.load_key("whisper.language", "auto")
            if whisper_language == 'auto':
                return self.config.load_key("whisper.detected_language", "English")
            return whisper_language
        except:
            return "English"
    
    def _get_joiner(self) -> str:
        """è·å–è¯­è¨€å¯¹åº”çš„è¿æ¥ç¬¦"""
        try:
            # æ— ç©ºæ ¼è¯­è¨€åˆ—è¡¨
            no_space_langs = self.config.load_key("language_split_without_space", [])
            if any(lang.lower() in self.language.lower() for lang in no_space_langs):
                return ""
            return " "
        except:
            return " "
    
    def _get_split_prompt(self, sentence: str, num_parts: int = 2, word_limit: int = 20) -> str:
        """ç”Ÿæˆåˆ†å‰²æç¤ºè¯"""
        return f"""
## Role
You are a professional Netflix subtitle splitter in **{self.language}**.

## Task
Split the given subtitle text into **{num_parts}** parts, each less than **{word_limit}** words.

1. Maintain sentence meaning coherence according to Netflix subtitle standards
2. MOST IMPORTANT: Keep parts roughly equal in length (minimum 3 words each)
3. Split at natural points like punctuation marks or conjunctions
4. If provided text is repeated words, simply split at the middle of the repeated words.

## Steps
1. Analyze the sentence structure, complexity, and key splitting challenges
2. Generate two alternative splitting approaches with [br] tags at split positions
3. Compare both approaches highlighting their strengths and weaknesses
4. Choose the best splitting approach

## Given Text
<split_this_sentence>
{sentence}
</split_this_sentence>

## Output in only JSON format and no other text
```json
{{
    "analysis": "Brief description of sentence structure, complexity, and key splitting challenges",
    "split1": "First splitting approach with [br] tags at split positions",
    "split2": "Alternative splitting approach with [br] tags at split positions",
    "assess": "Comparison of both approaches highlighting their strengths and weaknesses",
    "choice": "1 or 2"
}}
```

Note: Start you answer with ```json and end with ```, do not add any other text.
""".strip()
    
    def _find_split_positions(self, original: str, modified: str) -> List[int]:
        """æŸ¥æ‰¾åˆ†å‰²ä½ç½®"""
        split_positions = []
        parts = modified.split('[br]')
        start = 0
        
        for i in range(len(parts) - 1):
            max_similarity = 0
            best_split = None
            
            for j in range(start, len(original)):
                original_left = original[start:j]
                modified_left = self.joiner.join(parts[i].split())
                
                left_similarity = SequenceMatcher(None, original_left, modified_left).ratio()
                
                if left_similarity > max_similarity:
                    max_similarity = left_similarity
                    best_split = j
            
            if max_similarity < 0.9:
                print(f"âš ï¸  ä½ç›¸ä¼¼åº¦åˆ†å‰²ç‚¹: {max_similarity}")
            
            if best_split is not None:
                split_positions.append(best_split)
                start = best_split
            else:
                print(f"âš ï¸  æ— æ³•æ‰¾åˆ°ç¬¬{i+1}éƒ¨åˆ†çš„åˆé€‚åˆ†å‰²ç‚¹")
        
        return split_positions
    
    def _split_sentence(self, sentence: str, num_parts: int, 
                       word_limit: int = 20, index: int = -1) -> str:
        """ä½¿ç”¨GPTåˆ†å‰²å•ä¸ªå¥å­"""
        
        def valid_split(response_data):
            choice = response_data.get("choice", "1")
            split_key = f'split{choice}'
            if split_key not in response_data:
                return {"status": "error", "message": "Missing required key: `split`"}
            if "[br]" not in response_data[split_key]:
                return {"status": "error", "message": "Split failed, no [br] found"}
            return {"status": "success", "message": "Split completed"}
        
        # å¤šæ¬¡é‡è¯•
        for retry_attempt in range(self.retry_attempts):
            try:
                split_prompt = self._get_split_prompt(sentence, num_parts, word_limit)
                
                # æ·»åŠ é‡è¯•çš„éšæœºæ€§
                prompt_with_retry = split_prompt + " " * retry_attempt
                
                response_data = ask_gpt(
                    prompt_with_retry, 
                    resp_type='json', 
                    valid_def=valid_split, 
                    log_title='semantic_split',
                    cache_dir=self.output_dir / "gpt_log"
                )
                
                choice = response_data.get("choice", "1")
                best_split = response_data[f"split{choice}"]
                
                # æ‰¾åˆ°åˆ†å‰²ä½ç½®
                split_points = self._find_split_positions(sentence, best_split)
                
                # æ ¹æ®åˆ†å‰²ç‚¹åˆ†å‰²å¥å­
                result = sentence
                for i, split_point in enumerate(split_points):
                    if i == 0:
                        result = sentence[:split_point] + '\n' + sentence[split_point:]
                    else:
                        parts = result.split('\n')
                        last_part = parts[-1]
                        split_in_last = split_point - split_points[i-1]
                        parts[-1] = last_part[:split_in_last] + '\n' + last_part[split_in_last:]
                        result = '\n'.join(parts)
                
                if index != -1:
                    print(f"âœ… å¥å­ {index} æˆåŠŸåˆ†å‰²")
                    print(f"   åŸå¥: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
                    print(f"   åˆ†å‰²: {result.replace(chr(10), ' ||')}")
                
                return result
                
            except Exception as e:
                print(f"âš ï¸  å¥å­åˆ†å‰²å¤±è´¥ (index={index}, é‡è¯•{retry_attempt+1}/{self.retry_attempts}): {str(e)}")
                if retry_attempt == self.retry_attempts - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›åŸå¥
                    print(f"âŒ å¥å­ {index} åˆ†å‰²æœ€ç»ˆå¤±è´¥ï¼Œä½¿ç”¨åŸå¥")
                    return sentence
        
        return sentence
    
    def _tokenize_sentence(self, sentence: str) -> List[str]:
        """ç®€å•çš„åˆ†è¯å‡½æ•°"""
        # å¯¹äºæ— ç©ºæ ¼è¯­è¨€ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²ï¼›æœ‰ç©ºæ ¼è¯­è¨€æŒ‰å•è¯åˆ†å‰²
        if not self.joiner:  # æ— ç©ºæ ¼è¯­è¨€
            return list(sentence.strip())
        else:  # æœ‰ç©ºæ ¼è¯­è¨€
            return sentence.strip().split()
    
    def _process_sentences_sync(self, sentences: List[str]) -> List[str]:
        """åŒæ­¥åˆ†å‰²å¥å­"""
        new_sentences = []
        
        for index, sentence in enumerate(sentences):
            tokens = self._tokenize_sentence(sentence)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
            if len(tokens) > self.max_split_length:
                num_parts = math.ceil(len(tokens) / self.max_split_length)
                print(f"ğŸ”„ å¤„ç†å¥å­ {index}: {len(tokens)} ä¸ªè¯ -> åˆ†å‰²ä¸º {num_parts} éƒ¨åˆ†")
                
                split_result = self._split_sentence(
                    sentence, 
                    num_parts, 
                    self.max_split_length, 
                    index
                )
                
                if split_result and '\n' in split_result:
                    split_lines = split_result.strip().split('\n')
                    for line in split_lines:
                        if line.strip():
                            new_sentences.append(line.strip())
                else:
                    new_sentences.append(sentence)
            else:
                new_sentences.append(sentence)
        
        return new_sentences
    
    def split_sentences(self, sentences: List[str]) -> List[str]:
        """åˆ†å‰²å¥å­åˆ—è¡¨"""
        print(f"ğŸ¤– å¼€å§‹è¯­ä¹‰åˆ†å‰² {len(sentences)} ä¸ªå¥å­")
        print(f"   æœ€å¤§åˆ†å‰²é•¿åº¦: {self.max_split_length}")
        print(f"   è¯­è¨€: {self.language}")
        print(f"   å¤„ç†æ¨¡å¼: åŒæ­¥å¤„ç†")
        
        # åŒæ­¥å¤„ç†æ‰€æœ‰å¥å­
        result_sentences = self._process_sentences_sync(sentences)
        
        print(f"âœ… è¯­ä¹‰åˆ†å‰²å®Œæˆ: {len(sentences)} -> {len(result_sentences)}")
        return result_sentences
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†GPTå…¨å±€å®¢æˆ·ç«¯
            from modules.gpt import cleanup_global_client
            cleanup_global_client()
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†GPTå®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        self.cleanup()
        return False
    
    def split_file(self, input_file: str, output_file: str = None) -> str:
        """åˆ†å‰²æ–‡ä»¶ä¸­çš„å¥å­"""
        print("ğŸ¤– å¼€å§‹è¯­ä¹‰æ–‡æœ¬åˆ†å‰²...")
        
        # ç¡®ä¿è¾“å…¥æ–‡ä»¶è·¯å¾„æ­£ç¡®
        input_file = paths.get_filepath_by_default(input_file, self.output_dir)
        
        # è¯»å–è¾“å…¥æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                sentences = [line.strip() for line in f.readlines() if line.strip()]
        except Exception as e:
            raise FileNotFoundError(f"âŒ æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶ {input_file}: {str(e)}")
        
        # æ‰§è¡Œåˆ†å‰²
        split_sentences = self.split_sentences(sentences)
        
        # ä¿å­˜ç»“æœ
        if not output_file:
            output_file = paths.get_filepath_by_default("split_by_meaning.txt", self.output_dir)
        else:
            output_file = paths.get_filepath_by_default(output_file, output_base_dir=self.output_dir)
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                for sentence in split_sentences:
                    f.write(sentence + '\n')
        except Exception as e:
            raise RuntimeError(f"âŒ æ— æ³•ä¿å­˜ç»“æœæ–‡ä»¶ {output_file}: {str(e)}")
        
        print("âœ… è¯­ä¹‰åˆ†å‰²å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š åˆ†å‰²ç»Ÿè®¡: {len(sentences)} -> {len(split_sentences)} ä¸ªå¥å­")
        
        return str(output_file)


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # æµ‹è¯•è¯­ä¹‰åˆ†å‰²å™¨
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        input_file = "output/split_by_nlp.txt"
    
    try:
        splitter = SemanticSplitter()
        result_file = splitter.split_file(input_file)
        print(f"âœ… æµ‹è¯•å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc() 