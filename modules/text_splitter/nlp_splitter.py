"""
# ----------------------------------------------------------------------------
# NLPæ–‡æœ¬åˆ†å‰²å™¨
# 
# åŸºäºspaCyçš„å¤šå±‚NLPåˆ†å‰²ç­–ç•¥ï¼š
# 1. æ ‡ç‚¹ç¬¦å·åˆ†å‰²
# 2. é€—å·åˆ†å‰² 
# 3. è¿æ¥è¯åˆ†å‰²
# 4. é•¿å¥æ ¹æ®ä¾èµ–å…³ç³»åˆ†å‰²
# ----------------------------------------------------------------------------
"""

import os
from pathlib import Path
from typing import Optional

from .split_spacy import SpacySplitter, SplitterConfig
from modules.common_utils import paths


class NLPSplitter:
    """åŸºäºNLPçš„æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self, 
                 output_dir: str = "output",
                 enable_all_strategies: bool = True,
                 max_sentence_length: int = 60,
                 min_sentence_length: int = 3):
        """
        åˆå§‹åŒ–NLPåˆ†å‰²å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            enable_all_strategies: æ˜¯å¦å¯ç”¨æ‰€æœ‰åˆ†å‰²ç­–ç•¥
            max_sentence_length: æœ€å¤§å¥å­é•¿åº¦
            min_sentence_length: æœ€å°å¥å­é•¿åº¦
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # é…ç½®spaCyåˆ†å‰²å™¨
        self.config = SplitterConfig(
            output_dir=str(self.output_dir),
            output_file="split_by_nlp.txt",
            enable_mark_split=enable_all_strategies,
            enable_comma_split=enable_all_strategies,
            enable_connector_split=enable_all_strategies,
            enable_root_split=enable_all_strategies,
            max_sentence_length=max_sentence_length,
            min_sentence_length=min_sentence_length
        )
        
        self.splitter = SpacySplitter(self.config)
    
    def split_file(self, input_file: str, output_file: Optional[str] = None) -> str:
        """
        å¯¹æ–‡ä»¶è¿›è¡ŒNLPåˆ†å‰²
        
        Args:
            input_file: è¾“å…¥Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ†å‰²ç»“æœæ–‡ä»¶è·¯å¾„
        """
        print("ğŸ”§ å¼€å§‹NLPæ–‡æœ¬åˆ†å‰²...")
        
        # ç¡®ä¿è¾“å…¥æ–‡ä»¶è·¯å¾„æ­£ç¡®
        if not os.path.isabs(input_file):
            input_file = paths.get_filepath_by_default(input_file)
        
        if output_file is None:
            output_file = paths.get_filepath_by_default("split_by_nlp.txt", output_base_dir=self.output_dir)
        else:
            output_file = paths.get_filepath_by_default(output_file, output_base_dir=self.output_dir)
        
        # å¯åŠ¨åˆ†å‰²å™¨å¹¶å¤„ç†
        with self.splitter:
            result = self.splitter.process_file(input_file, output_file)
        
        if result.success:
            print("âœ… NLPåˆ†å‰²å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result.output_file}")
            print(f"ğŸ“Š åˆ†å‰²ç»Ÿè®¡: {result.final_sentence_count}ä¸ªå¥å­")
            return str(result.output_file)
        else:
            raise RuntimeError(f"âŒ NLPåˆ†å‰²å¤±è´¥: {result.error}")
    
    def split_sentences(self, sentences: list) -> list:
        """
        ç›´æ¥å¯¹å¥å­åˆ—è¡¨è¿›è¡Œåˆ†å‰²
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            
        Returns:
            åˆ†å‰²åçš„å¥å­åˆ—è¡¨
        """
        print("ğŸ”§ å¼€å§‹NLPå¥å­åˆ†å‰²...")
        
        # å¯åŠ¨åˆ†å‰²å™¨
        with self.splitter:
            nlp = self.splitter.runtime.get_nlp()
            
            # æ‰‹åŠ¨æ‰§è¡Œåˆ†å‰²æµç¨‹
            current_sentences = sentences
            
            # Step 1: Skip mark splitting for sentence list
            # Step 2: Comma splitting
            if self.config.enable_comma_split:
                from .split_spacy.strategies import CommaSplitter
                comma_splitter = CommaSplitter(self.config)
                current_sentences, _ = comma_splitter.process(current_sentences, nlp)
            
            # Step 3: Connector splitting
            if self.config.enable_connector_split:
                from .split_spacy.strategies import ConnectorSplitter
                connector_splitter = ConnectorSplitter(self.config)
                current_sentences, _ = connector_splitter.process(current_sentences, nlp)
            
            # Step 4: Root splitting
            if self.config.enable_root_split:
                from .split_spacy.strategies import RootSplitter
                root_splitter = RootSplitter(self.config)
                current_sentences, _ = root_splitter.process(current_sentences, nlp)
        
        print(f"âœ… NLPå¥å­åˆ†å‰²å®Œæˆ: {len(sentences)} -> {len(current_sentences)}")
        return current_sentences


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # æµ‹è¯•NLPåˆ†å‰²å™¨
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        input_file = "log/cleaned_chunks.xlsx"
    
    try:
        splitter = NLPSplitter()
        result_file = splitter.split_file(input_file)
        print(f"âœ… æµ‹è¯•å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc() 