"""
# ----------------------------------------------------------------------------
# æ–‡æœ¬åˆ†å‰²å™¨æ¨¡å— - è´Ÿè´£å°†è½¬å½•æ–‡æœ¬è¿›è¡Œæ™ºèƒ½åˆ†å‰²ä¼˜åŒ–
# 
# æ ¸å¿ƒåŠŸèƒ½ï¼š
# 1. NLPæ ‡ç‚¹ç¬¦å·åˆ†å‰²
# 2. æ™ºèƒ½é€—å·åˆ†å‰² 
# 3. å¥å­è¿æ¥ç¬¦åˆ†å‰²
# 4. åŸºäºè¯­ä¹‰çš„GPTæ™ºèƒ½åˆ†å‰²
# 5. é•¿å¥æ ¹æ®è¯­æ³•ç»“æ„åˆ†å‰²
# 
# è¾“å…¥ï¼šè½¬å½•ç»“æœExcelæ–‡ä»¶
# è¾“å‡ºï¼šå¤šå±‚çº§åˆ†å‰²ä¼˜åŒ–çš„æ–‡æœ¬æ–‡ä»¶
# ----------------------------------------------------------------------------
"""

import os
import re
import json
import math
import itertools
import warnings
from typing import List, Dict, Optional, Callable
from pathlib import Path
import concurrent.futures
from difflib import SequenceMatcher

warnings.filterwarnings("ignore", category=FutureWarning)


class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨ç±» - å°è£…æ–‡æœ¬åˆ†å‰²çš„æ‰€æœ‰åŠŸèƒ½"""
    
    def __init__(self,
                 input_file: str = 'output/log/2_cleaned_chunks.xlsx',
                 output_dir: str = 'output/log',
                 language: str = 'en',
                 max_split_length: int = 20,
                 max_workers: int = 4):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        
        Args:
            input_file: è¾“å…¥çš„è½¬å½•ç»“æœæ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•
            language: è¯­è¨€ä»£ç  (en, zh, frç­‰)
            max_split_length: æœ€å¤§åˆ†å‰²é•¿åº¦
            max_workers: å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°
        """
        self.input_file = Path(input_file)
        self.output_dir = Path(output_dir)
        self.language = language
        self.max_split_length = max_split_length
        self.max_workers = max_workers
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.mark_split_file = self.output_dir / '3_1_split_by_mark.txt'
        self.comma_split_file = self.output_dir / '3_1_split_by_comma.txt'
        self.connector_split_file = self.output_dir / '3_1_split_by_connector.txt'
        self.nlp_split_file = self.output_dir / '3_1_split_by_nlp.txt'
        self.meaning_split_file = self.output_dir / '3_2_split_by_meaning.txt'
        
        # è¯­è¨€ç›¸å…³é…ç½®
        self.language_configs = {
            'en': {'joiner': ' ', 'spacy_model': 'en_core_web_md'},
            'zh': {'joiner': '', 'spacy_model': 'zh_core_web_sm'},
            'fr': {'joiner': ' ', 'spacy_model': 'fr_core_news_sm'},
            'de': {'joiner': ' ', 'spacy_model': 'de_core_news_sm'},
            'es': {'joiner': ' ', 'spacy_model': 'es_core_news_sm'},
            'ja': {'joiner': '', 'spacy_model': 'ja_core_news_sm'},
        }
        
        self.joiner = self.language_configs.get(language, {'joiner': ' '})['joiner']
        self.spacy_model = self.language_configs.get(language, {'spacy_model': 'en_core_web_md'})['spacy_model']
        
        # æ‡’åŠ è½½spaCyå’Œpandas
        self._nlp = None
        self._pd = None
        self._ask_gpt_func = None
    
    def _get_nlp(self):
        """æ‡’åŠ è½½spaCyæ¨¡å‹"""
        if self._nlp is None:
            try:
                import spacy
                print(f"ğŸ”„ æ­£åœ¨åŠ è½½spaCyæ¨¡å‹: {self.spacy_model}")
                try:
                    self._nlp = spacy.load(self.spacy_model)
                    print(f"âœ… spaCyæ¨¡å‹åŠ è½½æˆåŠŸ")
                except IOError:
                    print(f"âš ï¸  æ¨¡å‹ {self.spacy_model} æœªæ‰¾åˆ°, æ­£åœ¨ä¸‹è½½...")
                    spacy.cli.download(self.spacy_model)
                    self._nlp = spacy.load(self.spacy_model)
                    print(f"âœ… spaCyæ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
            except ImportError:
                raise ImportError("âŒ æœªæ‰¾åˆ°spaCyåº“, è¯·å®‰è£…: pip install spacy")
        return self._nlp
    
    def _get_pandas(self):
        """æ‡’åŠ è½½pandas"""
        if self._pd is None:
            try:
                import pandas as pd
                self._pd = pd
            except ImportError:
                raise ImportError("âŒ æœªæ‰¾åˆ°pandasåº“, è¯·å®‰è£…: pip install pandas")
        return self._pd
    
    def set_gpt_function(self, ask_gpt_func: Callable):
        """
        è®¾ç½®GPTè°ƒç”¨å‡½æ•°
        
        Args:
            ask_gpt_func: GPT APIè°ƒç”¨å‡½æ•°
        """
        self._ask_gpt_func = ask_gpt_func
        print("âœ… GPTå‡½æ•°å·²è®¾ç½®")
    
    def load_transcription_data(self) -> List[str]:
        """
        åŠ è½½è½¬å½•æ•°æ®
        
        Returns:
            æ–‡æœ¬åˆ—è¡¨
        """
        print(f"ğŸ“– æ­£åœ¨åŠ è½½è½¬å½•æ•°æ®: {self.input_file}")
        
        if not self.input_file.exists():
            raise FileNotFoundError(f"âŒ è½¬å½•æ–‡ä»¶ä¸å­˜åœ¨: {self.input_file}")
        
        try:
            pd = self._get_pandas()
            chunks = pd.read_excel(self.input_file)
            
            if 'text' not in chunks.columns:
                raise ValueError("âŒ Excelæ–‡ä»¶ç¼ºå°‘'text'åˆ—")
            
            # æ¸…ç†æ–‡æœ¬æ•°æ®
            chunks['text'] = chunks['text'].apply(lambda x: str(x).strip('"').strip())
            text_list = chunks['text'].tolist()
            
            # è¿‡æ»¤ç©ºæ–‡æœ¬
            text_list = [text for text in text_list if text.strip()]
            
            print(f"âœ… åŠ è½½äº†{len(text_list)}æ¡æ–‡æœ¬è®°å½•")
            return text_list
            
        except Exception as e:
            print(f"âŒ åŠ è½½è½¬å½•æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def split_by_punctuation_marks(self, text_list: List[str]) -> List[str]:
        """
        ä½¿ç”¨æ ‡ç‚¹ç¬¦å·è¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text_list: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åˆ†å‰²åçš„å¥å­åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æ ‡ç‚¹ç¬¦å·åˆ†å‰²...")
        
        try:
            nlp = self._get_nlp()
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            combined_text = self.joiner.join(text_list)
            
            # ä½¿ç”¨spaCyè¿›è¡Œå¥å­åˆ†å‰²
            doc = nlp(combined_text)
            
            if not doc.has_annotation("SENT_START"):
                print("âš ï¸  spaCyæ¨¡å‹ä¸æ”¯æŒå¥å­åˆ†å‰², ä½¿ç”¨ç®€å•åˆ†å‰²")
                return self._simple_sentence_split(combined_text)
            
            sentences_by_mark = []
            current_sentence = []
            
            # å¤„ç†å¥å­åˆ†å‰², ç‰¹åˆ«å¤„ç†ç ´æŠ˜å·å’Œçœç•¥å·
            for sent in doc.sents:
                text = sent.text.strip()
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸å‰ä¸€å¥åˆå¹¶
                if current_sentence and (
                    text.startswith('-') or 
                    text.startswith('...') or
                    current_sentence[-1].endswith('-') or
                    current_sentence[-1].endswith('...')
                ):
                    current_sentence.append(text)
                else:
                    if current_sentence:
                        sentences_by_mark.append(' '.join(current_sentence))
                        current_sentence = []
                    current_sentence.append(text)
            
            # æ·»åŠ æœ€åä¸€ä¸ªå¥å­
            if current_sentence:
                sentences_by_mark.append(' '.join(current_sentence))
            
            # å¤„ç†å•ç‹¬çš„æ ‡ç‚¹ç¬¦å·è¡Œï¼ˆä¸­æ–‡ã€æ—¥æ–‡ç­‰ï¼‰
            final_sentences = []
            for i, sentence in enumerate(sentences_by_mark):
                if i > 0 and sentence.strip() in [',', '.', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼']:
                    # ä¸å‰ä¸€å¥åˆå¹¶
                    if final_sentences:
                        final_sentences[-1] += sentence
                else:
                    final_sentences.append(sentence)
            
            print(f"âœ… æ ‡ç‚¹ç¬¦å·åˆ†å‰²å®Œæˆ, å…±{len(final_sentences)}ä¸ªå¥å­")
            return final_sentences
            
        except Exception as e:
            print(f"âŒ æ ‡ç‚¹ç¬¦å·åˆ†å‰²å¤±è´¥: {str(e)}")
            # å›é€€åˆ°ç®€å•åˆ†å‰²
            return self._simple_sentence_split(self.joiner.join(text_list))
    
    def _simple_sentence_split(self, text: str) -> List[str]:
        """ç®€å•çš„å¥å­åˆ†å‰²å¤‡ç”¨æ–¹æ¡ˆ"""
        print("ğŸ“ ä½¿ç”¨ç®€å•å¥å­åˆ†å‰²...")
        
        # åŸºäºå¸¸è§å¥å­ç»“æŸç¬¦åˆ†å‰²
        sentence_enders = r'[.!?ã€‚ï¼ï¼Ÿ]+\s+'
        sentences = re.split(sentence_enders, text)
        
        # æ¸…ç†ç©ºå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _is_valid_phrase(self, phrase) -> bool:
        """
        æ£€æŸ¥çŸ­è¯­æ˜¯å¦æœ‰æ•ˆï¼ˆåŒ…å«ä¸»è¯­å’Œè°“è¯­ï¼‰
        
        Args:
            phrase: spaCy Docå¯¹è±¡
            
        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆçŸ­è¯­
        """
        has_subject = any(
            token.dep_ in ["nsubj", "nsubjpass"] or token.pos_ == "PRON" 
            for token in phrase
        )
        has_verb = any(
            token.pos_ in ["VERB", "AUX"] 
            for token in phrase
        )
        return has_subject and has_verb
    
    def _analyze_comma_split(self, start: int, doc, token) -> bool:
        """
        åˆ†æé€—å·ä½ç½®æ˜¯å¦é€‚åˆåˆ†å‰²
        
        Args:
            start: èµ·å§‹ä½ç½®
            doc: spaCy Docå¯¹è±¡
            token: å½“å‰token
            
        Returns:
            æ˜¯å¦é€‚åˆåˆ†å‰²
        """
        # è·å–å·¦å³çŸ­è¯­
        left_phrase = doc[max(start, token.i - 9):token.i]
        right_phrase = doc[token.i + 1:min(len(doc), token.i + 10)]
        
        # æ£€æŸ¥å³ä¾§çŸ­è¯­çš„æœ‰æ•ˆæ€§
        suitable_for_splitting = self._is_valid_phrase(right_phrase)
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶æ£€æŸ¥è¯æ•°
        left_words = [t for t in left_phrase if not t.is_punct]
        right_words = list(itertools.takewhile(lambda t: not t.is_punct, right_phrase))
        
        # ç¡®ä¿å·¦å³ä¸¤è¾¹éƒ½æœ‰è¶³å¤Ÿçš„è¯
        if len(left_words) <= 3 or len(right_words) <= 3:
            suitable_for_splitting = False
        
        return suitable_for_splitting
    
    def split_by_commas(self, sentences: List[str]) -> List[str]:
        """
        æ™ºèƒ½é€—å·åˆ†å‰²
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            
        Returns:
            é€—å·åˆ†å‰²åçš„å¥å­åˆ—è¡¨
        """
        print("ğŸ“ å¼€å§‹æ™ºèƒ½é€—å·åˆ†å‰²...")
        
        try:
            nlp = self._get_nlp()
            all_split_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                doc = nlp(sentence)
                split_sentences = []
                start = 0
                
                for i, token in enumerate(doc):
                    if token.text in [",", "ï¼Œ"]:
                        suitable_for_splitting = self._analyze_comma_split(start, doc, token)
                        
                        if suitable_for_splitting:
                            split_part = doc[start:token.i].text.strip()
                            split_sentences.append(split_part)
                            print(f"âœ‚ï¸  é€—å·åˆ†å‰²: ...{doc[start:token.i][-20:]} | {doc[token.i + 1:][:20]}...")
                            start = token.i + 1
                
                # æ·»åŠ æœ€åä¸€éƒ¨åˆ†
                if start < len(doc):
                    split_sentences.append(doc[start:].text.strip())
                
                # å¦‚æœæ²¡æœ‰åˆ†å‰², ä¿ç•™åŸå¥
                if not split_sentences:
                    split_sentences = [sentence]
                
                all_split_sentences.extend(split_sentences)
            
            print(f"âœ… é€—å·åˆ†å‰²å®Œæˆ, å…±{len(all_split_sentences)}ä¸ªç‰‡æ®µ")
            return all_split_sentences
            
        except Exception as e:
            print(f"âŒ é€—å·åˆ†å‰²å¤±è´¥: {str(e)}")
            return sentences
    
    def split_by_semantic_meaning(self, sentences: List[str]) -> List[str]:
        """
        åŸºäºè¯­ä¹‰çš„æ™ºèƒ½åˆ†å‰²
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            
        Returns:
            è¯­ä¹‰åˆ†å‰²åçš„å¥å­åˆ—è¡¨
        """
        print("ğŸ§  å¼€å§‹è¯­ä¹‰æ™ºèƒ½åˆ†å‰²...")
        
        if self._ask_gpt_func is None:
            print("âš ï¸  æœªè®¾ç½®GPTå‡½æ•°, è·³è¿‡è¯­ä¹‰åˆ†å‰²")
            return sentences
        
        try:
            nlp = self._get_nlp()
            
            # å¹¶è¡Œå¤„ç†è¯­ä¹‰åˆ†å‰²
            new_sentences = self._parallel_semantic_split(sentences, nlp)
            
            print(f"âœ… è¯­ä¹‰åˆ†å‰²å®Œæˆ, å…±{len(new_sentences)}ä¸ªå¥å­")
            return new_sentences
            
        except Exception as e:
            print(f"âŒ è¯­ä¹‰åˆ†å‰²å¤±è´¥: {str(e)}")
            return sentences
    
    def _tokenize_sentence(self, sentence: str) -> List[str]:
        """
        å¯¹å¥å­è¿›è¡Œåˆ†è¯
        
        Args:
            sentence: å¥å­
            
        Returns:
            è¯æ±‡åˆ—è¡¨
        """
        try:
            nlp = self._get_nlp()
            doc = nlp(sentence)
            return [token.text for token in doc]
        except:
            # ç®€å•åˆ†è¯å¤‡ç”¨æ–¹æ¡ˆ
            return sentence.split()
    
    def _split_single_sentence(self, sentence: str, num_parts: int = 2, index: int = -1) -> str:
        """
        ä½¿ç”¨GPTåˆ†å‰²å•ä¸ªå¥å­
        
        Args:
            sentence: å¾…åˆ†å‰²çš„å¥å­
            num_parts: åˆ†å‰²æˆçš„éƒ¨åˆ†æ•°
            index: å¥å­ç´¢å¼•
            
        Returns:
            åˆ†å‰²åçš„å¥å­ï¼ˆç”¨\\nåˆ†éš”ï¼‰
        """
        if self._ask_gpt_func is None:
            return sentence
        
        try:
            # æ„å»ºåˆ†å‰²æç¤ºè¯
            split_prompt = self._get_split_prompt(sentence, num_parts, self.max_split_length)
            
            # è°ƒç”¨GPTè¿›è¡Œåˆ†å‰²
            response_data = self._ask_gpt_func(
                split_prompt,
                resp_type='json',
                log_title='semantic_split'
            )
            
            # è§£æå“åº”
            choice = response_data.get("choice", "1")
            best_split = response_data.get(f"split{choice}", sentence)
            
            # æŸ¥æ‰¾åˆ†å‰²ç‚¹
            split_points = self._find_split_positions(sentence, best_split)
            
            # åº”ç”¨åˆ†å‰²ç‚¹
            result = self._apply_split_points(sentence, split_points)
            
            if index >= 0:
                print(f"âœ… å¥å­{index}è¯­ä¹‰åˆ†å‰²å®Œæˆ")
            
            return result
            
        except Exception as e:
            print(f"âš ï¸  å¥å­åˆ†å‰²å¤±è´¥: {str(e)}")
            return sentence
    
    def _get_split_prompt(self, sentence: str, num_parts: int, word_limit: int) -> str:
        """ç”Ÿæˆåˆ†å‰²æç¤ºè¯"""
        return f"""
## Role
You are a professional Netflix subtitle splitter in **{self.language}**.

## Task
Split the given subtitle text into **{num_parts}** parts, each less than **{word_limit}** words.

1. Maintain sentence meaning coherence according to Netflix subtitle standards
2. MOST IMPORTANT: Keep parts roughly equal in length (minimum 3 words each)
3. Split at natural points like punctuation marks or conjunctions
4. If provided text is repeated words, simply split at the middle of the repeated words.

## Steps
1. Analyze the sentence structure, complexity, and key splitting challenges
2. Generate two alternative splitting approaches with [br] tags at split positions
3. Compare both approaches highlighting their strengths and weaknesses
4. Choose the best splitting approach

## Given Text
<split_this_sentence>
{sentence}
</split_this_sentence>

## Output in only JSON format and no other text
```json
{{
    "analysis": "Brief description of sentence structure, complexity, and key splitting challenges",
    "split1": "First splitting approach with [br] tags at split positions",
    "split2": "Alternative splitting approach with [br] tags at split positions",
    "assess": "Comparison of both approaches highlighting their strengths and weaknesses",
    "choice": "1 or 2"
}}
```

Note: Start you answer with ```json and end with ```, do not add any other text.
""".strip()
    
    def _find_split_positions(self, original: str, modified: str) -> List[int]:
        """æŸ¥æ‰¾åˆ†å‰²ä½ç½®"""
        split_positions = []
        parts = modified.split('[br]')
        start = 0
        
        for i in range(len(parts) - 1):
            max_similarity = 0
            best_split = None
            
            for j in range(start, len(original)):
                original_left = original[start:j]
                modified_left = self.joiner.join(parts[i].split())
                
                left_similarity = SequenceMatcher(None, original_left, modified_left).ratio()
                
                if left_similarity > max_similarity:
                    max_similarity = left_similarity
                    best_split = j
            
            if max_similarity < 0.9:
                print(f"âš ï¸  åˆ†å‰²ç‚¹ç›¸ä¼¼åº¦è¾ƒä½: {max_similarity:.3f}")
            
            if best_split is not None:
                split_positions.append(best_split)
                start = best_split
        
        return split_positions
    
    def _apply_split_points(self, sentence: str, split_points: List[int]) -> str:
        """åº”ç”¨åˆ†å‰²ç‚¹"""
        if not split_points:
            return sentence
        
        result = sentence
        for i, split_point in enumerate(split_points):
            if i == 0:
                result = sentence[:split_point] + '\n' + sentence[split_point:]
            else:
                parts = result.split('\n')
                last_part = parts[-1]
                offset = split_point - split_points[i-1]
                parts[-1] = last_part[:offset] + '\n' + last_part[offset:]
                result = '\n'.join(parts)
        
        return result
    
    def _parallel_semantic_split(self, sentences: List[str], nlp) -> List[str]:
        """å¹¶è¡Œè¯­ä¹‰åˆ†å‰²å¤„ç†"""
        new_sentences = [None] * len(sentences)
        futures = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            for index, sentence in enumerate(sentences):
                tokens = self._tokenize_sentence(sentence)
                num_parts = math.ceil(len(tokens) / self.max_split_length)
                
                if len(tokens) > self.max_split_length:
                    future = executor.submit(
                        self._split_single_sentence, 
                        sentence, 
                        num_parts, 
                        index
                    )
                    futures.append((future, index))
                else:
                    new_sentences[index] = [sentence]
            
            # æ”¶é›†ç»“æœ
            for future, index in futures:
                try:
                    split_result = future.result()
                    if split_result and '\n' in split_result:
                        split_lines = split_result.strip().split('\n')
                        new_sentences[index] = [line.strip() for line in split_lines]
                    else:
                        new_sentences[index] = [sentences[index]]
                except Exception as e:
                    print(f"âš ï¸  å¥å­{index}åˆ†å‰²å¤±è´¥: {str(e)}")
                    new_sentences[index] = [sentences[index]]
        
        # å±•å¹³ç»“æœ
        return [sentence for sublist in new_sentences for sentence in sublist if sentence]
    
    def save_split_results(self, sentences: List[str], stage: str) -> str:
        """
        ä¿å­˜åˆ†å‰²ç»“æœ
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            stage: é˜¶æ®µåç§° ('mark', 'comma', 'connector', 'nlp', 'meaning')
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        file_map = {
            'mark': self.mark_split_file,
            'comma': self.comma_split_file,
            'connector': self.connector_split_file,
            'nlp': self.nlp_split_file,
            'meaning': self.meaning_split_file
        }
        
        output_file = file_map.get(stage, self.output_dir / f'split_{stage}.txt')
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                for sentence in sentences:
                    if sentence.strip():
                        f.write(sentence.strip() + '\n')
            
            print(f"ğŸ’¾ {stage}åˆ†å‰²ç»“æœå·²ä¿å­˜: {output_file}")
            return str(output_file)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜{stage}åˆ†å‰²ç»“æœå¤±è´¥: {str(e)}")
            raise
    
    def process_complete_split(self, 
                             enable_comma_split: bool = True,
                             enable_semantic_split: bool = True) -> str:
        """
        å®Œæ•´çš„æ–‡æœ¬åˆ†å‰²å¤„ç†æµç¨‹
        
        Args:
            enable_comma_split: æ˜¯å¦å¯ç”¨é€—å·åˆ†å‰²
            enable_semantic_split: æ˜¯å¦å¯ç”¨è¯­ä¹‰åˆ†å‰²
            
        Returns:
            æœ€ç»ˆåˆ†å‰²ç»“æœæ–‡ä»¶è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´æ–‡æœ¬åˆ†å‰²æµç¨‹...")
        
        try:
            # 1. åŠ è½½è½¬å½•æ•°æ®
            text_list = self.load_transcription_data()
            
            # 2. æ ‡ç‚¹ç¬¦å·åˆ†å‰²
            sentences = self.split_by_punctuation_marks(text_list)
            self.save_split_results(sentences, 'mark')
            
            # 3. é€—å·åˆ†å‰²ï¼ˆå¯é€‰ï¼‰
            if enable_comma_split:
                sentences = self.split_by_commas(sentences)
                self.save_split_results(sentences, 'comma')
            
            # åˆå¹¶ä¸ºNLPåˆ†å‰²ç»“æœ
            self.save_split_results(sentences, 'nlp')
            
            # 4. è¯­ä¹‰åˆ†å‰²ï¼ˆå¯é€‰ï¼‰
            if enable_semantic_split:
                sentences = self.split_by_semantic_meaning(sentences)
                final_file = self.save_split_results(sentences, 'meaning')
            else:
                final_file = str(self.nlp_split_file)
            
            print("ğŸ‰ æ–‡æœ¬åˆ†å‰²æµç¨‹å®Œæˆï¼")
            print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(sentences)}ä¸ªåˆ†å‰²ç‰‡æ®µ")
            
            return final_file
            
        except Exception as e:
            print(f"ğŸ’¥ æ–‡æœ¬åˆ†å‰²æµç¨‹å¤±è´¥: {str(e)}")
            raise


# ----------------------------------------------------------------------------
# ç‹¬ç«‹è¿è¡Œæµ‹è¯•
# ----------------------------------------------------------------------------
if __name__ == '__main__':
    import sys
    
    # åˆ›å»ºåˆ†å‰²å™¨å®ä¾‹
    splitter = TextSplitter(
        language='en',
        max_split_length=20,
        max_workers=2
    )
    
    # æµ‹è¯•å‚æ•°
    test_with_gpt = '--gpt' in sys.argv
    
    if test_with_gpt:
        print("âš ï¸  æ³¨æ„: éœ€è¦æä¾›GPTå‡½æ•°æ‰èƒ½è¿›è¡Œè¯­ä¹‰åˆ†å‰²æµ‹è¯•")
        # splitter.set_gpt_function(your_gpt_function)
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not splitter.input_file.exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {splitter.input_file}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡ŒéŸ³é¢‘è½¬å½•å™¨ç”Ÿæˆè½¬å½•æ–‡ä»¶")
            sys.exit(1)
        
        # è¿è¡Œå®Œæ•´åˆ†å‰²æµç¨‹
        print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬åˆ†å‰²æµç¨‹...")
        
        final_file = splitter.process_complete_split(
            enable_comma_split=True,
            enable_semantic_split=test_with_gpt
        )
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ æœ€ç»ˆæ–‡ä»¶: {final_file}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        if Path(final_file).exists():
            with open(final_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()[:10]  # æ˜¾ç¤ºå‰10è¡Œ
            
            print(f"\nğŸ“‹ ç»“æœé¢„è§ˆ (å‰{len(lines)}è¡Œ):")
            for i, line in enumerate(lines, 1):
                print(f"  {i:2d}. {line.strip()}")
            
            if len(lines) == 10:
                print("     ...")
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        sys.exit(1) 